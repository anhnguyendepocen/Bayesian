CHAPTER 4: Linear regression: before and after fitting the modelIt is not always appropriate to fit a classical linear regression model using data in their raw form. As we discuss in Sections 4.1 and 4.4, linear and logarithmic transformations can sometimes help in the interpretation of the model. Nonlinear transformations of the data are sometimes necessary to more closely satisfy additivity and linearity assumptions, which in turn should improve the fit and predictive power of the model. Section 4.5 presents some other univariate transformations that are occasionally useful. We have already discussed interactions in Section 3.3, and in Section 4.6 we consider other techniques for combining input variables.
4.1 Linear transformationsLinear transformations do not affect the fit of a classical regression model, and they do not affect predictions: the changes in the inputs and the coefficients cancel in forming the predicted value Xβ.1 However, well-chosen linear transformation can improve interpretability of coefficients and make a fitted model easier to understand. We saw in Chapter 3 how linear transformations can help with the interpretation of the intercept; this section provides examples involving the interpretation of the other coefficients in the model.Scaling of predictors and regression coefficients. The regression coefficient βj represents the average difference in y comparing units that differ by 1 unit on the jth predictor and are otherwise identical. In some cases, though, a difference of 1 unit on the x-scale is not the most relevant comparison. Consider, for example, a model fit to data we downloaded from a survey of adult Americans in 1994 that predicts their earnings (in dollars) given their height (in inches) and sex (coded as 1 for men and 2 for women):earnings = −61000 + 1300 · height + error, (4.1)with a residual standard deviation of 19000. (A linear model is not really appropriate for these data, as we shall discuss soon, but we’ll stick with the simple example for introducing the concept of linear transformations.)Figure 4.1 shows the regression line and uncertainty on a scale with the x-axis extended to zero to display the intercept—the point on the y-axis where the line crosses zero. The estimated intercept of −61000 has little meaning since it corresponds to the predicted earnings for a person of zero height.Now consider the following alternative forms of the model:earnings = −61000 + 51 · height (in millimeters) + error earnings = −61000 + 81000000 · height (in miles) + error.How important is height? While $51 does not seem to matter very much, $81,000,0001 In contrast, in a multilevel model, linear transformations can change the fit of a model and its predictions, as we explain in Section 13.6.Fitted linear model x−axis extended to 0Figure 4.1 Regression of earnings on height, earnings = −61000 + 1300 · height, with solid line showing the fitted regression model and light lines indicating uncertainty in the fitted regression. In the plot on the right, the x-scale is extended to zero to reveal the intercept of the regression line.is a lot. Yet, both these equations reflect the same underlying information. To understand these coefficients better, we need some sense of the variation in height in the population to which we plan to apply the model. One approach is to consider the standard deviation of heights in the data, which is 3.8 inches (or 97 millimeters, or 0.000061 miles). The expected difference in earnings corresponding to a 3.8-inch difference in height is $1300 · 3.8 = $51 · 97 = $81000000 · 0.000061 = $4900, which is reasonably large but much smaller than the residual standard deviation of $19000 unexplained by the regression.Standardization using z-scoresAnother way to scale the coefficients is to standardize the predictor by subtracting the mean and dividing by the standard deviation to yield a “z-score.” In this example, height would be replaced by z.height = (height − 66.9)/3.8, and the coefficient for z.height will be 4900. Then coefficients are interpreted in units of standard deviations with respect to the corresponding predictor just as they were, after the fact, in the previous example. In addition, standardizing predictors using z-scores will change our interpretation of the intercept to the mean of y when all predictor values are at their mean values.We actually prefer to divide by 2 standard deviations to allow inferences to be more consistent with those for binary inputs, as we discuss in Section 4.2.Standardization using reasonable scalesIt is often useful to keep inputs on familiar scales such as inches, dollars, or years, but making convenient rescalings to aid in the interpretability of coefficients. For example, we might work with income/$10000 or age/10.For another example, in some surveys, party identification is on a 1–7 scale, from strong Republican to strong Democrat. The rescaled variable (PID − 4)/2, equals −1 for Republicans, 0 for moderates, and +1 for Democrats, and so the coefficient on this variable is directly interpretable.4.2 Centering and standardizing, especially for models with interactionsFigure 4.1b illustrates the difficulty of interpreting the intercept term in a regression in a setting where it does not make sense to consider predictors set to zero. More generally, similar challenges arise in interpreting coefficients in models with interactions, as we saw in Section 3.3 with the following model:lm(formula = kid.score coef.est (Intercept) -11.5 mom.hs 51.3 mom.iq 1.1 mom.hs:mom.iq -0.5~ mom.hs + mom.iq + mom.hs:mom.iq) coef.se13.8 15.3 0.2 0.2R outputn = 434, k = 4residual sd = 18.0, R-Squared = 0.23The coefficient on mom.hs is 51.3—does this mean that children with mothers who graduated from high school do, on average, 51.3 points better on their tests? No. The model includes an interaction, and 51.3 is the predicted difference for kids that differ in mom.hs, among those with mom.iq = 0. Since mom.iq is never even close to zero (see Figure 3.4 on page 35), the comparison at zero, and thus the coefficient of 51.3, is essentially meaningless.Similarly, the coefficient of 1.1 for “main effect” of mom.iq is the slope for this variable, among those children for whom mom.hs = 0. This is less of a stretch (since mom.hs actually does equal zero for many of the cases in the data; see Figure 3.1 on page 32) but still can be somewhat misleading since mom.hs = 0 is at the edge of the data.Centering by subtracting the mean of the dataWe can simplify the interpretation of the regression model by first subtracting the mean of each input variable:c.mom.hs <- mom.hs - mean(mom.hs) c.mom.iq <- mom.iq - mean(mom.iq)The resulting regression is easier to interpret, with each main effect corresponding to a predictive difference with the other input at its average value:lm(formula = kid.score ~ c.mom.hs + c.mom.iq + c.mom.hs:c.mom.iq) coef.est coef.seR codeR output(Intercept) c.mom.hsc.mom.iq c.mom.hs:c.mom.iq87.6 0.9 2.8 2.4 0.6 0.1The residual standard deviation and R2 do not change—linear transformation of the predictors does not affect the fit of a classical regression model—and the coefficient and standard error of the interaction do not change, but the main effects and the intercept move a lot and are now interpretable based on comparison to the mean of the data.-0.5 0.2 residual sd = 18.0, R-Squared = 0.23n = 434, k = 4
R codeR outputUsing a conventional centering pointAnother option is to center based on an understandable reference point, for example, the midpoint of the range for mom.hs and the population average IQ:c2.mom.hs <- mom.hs - 0.5 c2.mom.iq <- mom.iq - 100In this parameterization, the coefficient of c2.mom.hs is the average predictive difference between a child with mom.hs = 1 and mom.hs = 0, for those children with mom.iq = 100. Similarly, the coefficient of c2.mom.iq corresponds to a comparison for the case mom.hs = 0.5, which includes no actual data but represents a midpoint of the range.R codeR outputn = 434, k = 4residual sd = 18.0, R-Squared = 0.23Once again, the residual standard deviation, R2, and coefficient for the interaction have not changed. The intercept and main effect have changed very little, because the points 0.5 and 100 happen to be close to the mean of mom.hs and mom.iq in the data.Standardizing by subtracting the mean and dividing by 2 standard deviationsCentering helped us interpret the main effects in the regression, but it still leaves us with a scaling problem. The coefficient of mom.hs is much larger than that of mom.iq, but this is misleading, considering that we are comparing the complete change in one variable (mother completed high school or not) to a mere 1-point change in mother’s IQ, which is not much at all (see Figure 3.4 on page 35).A natural step is to scale the predictors by dividing by 2 standard deviations—we shall explain shortly why we use 2 rather than 1—so that a 1-unit change in the rescaled predictor corresponds to a change from 1 standard deviation below the mean, to 1 standard deviation above. Here are the rescaled predictors in the child testing example:z.mom.hs <- (mom.hs - mean(mom.hs))/(2*sd(mom.hs)) z.mom.iq <- (mom.iq - mean(mom.iq))/(2*sd(mom.iq))We can now interpret all the coefficients on a roughly common scale (except for the intercept, which now corresponds to the average predicted outcome with all inputs at their mean):lm(formula = kid.score ~ z.mom.hs + z.mom.iq + z.mom.hs:z.mom.iq) coef.est coef.se(Intercept) 87.6 0.9 z.mom.hs 2.3 2.0 z.mom.iq 17.7 1.8 z.mom.hs:z.mom.iq -11.9 4.0n = 434, k = 4residual sd = 18.0, R-Squared = 0.23lm(formula = kid.score ~ c2.mom.hs coef.est coef.se (Intercept) 86.8 1.2 c2.mom.hs 2.8 2.4 c2.mom.iq 0.7 0.1 c2.mom.hs:c2.mom.iq -0.5 0.2+ c2.mom.iq + c2.mom.hs:c2.mom.iq)
Why scale by 2 standard deviations?We divide by 2 standard deviations rather than 1 to maintain coherence when considering binary input variables. To see this, consider the simplest binary x variablewhich takes on the values 0 and 1, each with probability 0.5. The standard deviation of x is then√0.5 · 0.5 = 0.5, and so the standardized variable, (x − μx)/(2σx),takes on the values ±0.5, and its coefficient reflects comparisons between x = 0 andx = 1. In contrast, if we had divided by 1 standard deviation, the rescaled variabletakes on the values ±1, and its coefficient corresponds to half the difference betweenthe two possible values of x. This identity is close to precise for binary inputs evenwhen the frequencies are not exactly equal, since p(1 − p) ≈ 0.5 when p is not too far from 0.5.In a complicated regression with many predictors, it can make sense to leave binary inputs as is, and linearly transform continuous inputs, possibly by scaling using the standard deviation. In this case, dividing by 2 standard deviations ensures a rough comparability in the coefficients. In our children’s testing example, the predictive difference corresponding to 2 standard deviations of mother’s IQ is clearly much higher than the comparison of mothers with and without a high school education.Multiplying each regression coefficient by 2 standard deviations of its predictorFor models with no interactions, a procedure that is equivalent to centering and rescaling is to leave the regression predictors as is, and then create rescaled regression coefficients by multiplying each β by two times the standard deviation of its corresponding x. This gives a sense of the importance of each variable, controlling for all the others in the linear model. As noted, scaling by 2 (rather than 1) standard deviations allows these scaled coefficients to be comparable to unscaled coefficients for binary predictors.4.3 Correlation and “regression to the mean”Consider a regression with a single predictor (in addition to the constant term); thus, y = a+bx+error. If both x and y are standardized—that is, if they are defined as x <- (x-mean(x))/sd(x) and y <- (y-mean(y))/sd(y)—then the regression intercept is zero and the slope is simply the correlation between x and y. Thus, the slope of a regression of two standardized variables must always be between −1 and 1, or, to put it another way, if a regression slope is more than 1 or less than −1, the variance of y must exceed that of x. In general, the slope of a regression with one predictor is b = ρσy/σx, where ρ is the correlation between the two variables and σx and σy are the standard deviations of x and y.The principal components line and the regression lineSome of the confusing aspects of regression can be understood in the simple case of standardized variables. Figure 4.2 shows a simulated-data example of standardized variables with correlation (and thus regression slope) 0.5. The left plot shows the principal component line, which goes closest through the cloud of points, in the sense of minimizing the sum of squared Euclidean distances between the points and the line. The principal component line in this case is simply y = x.The right plot in Figure 4.2 shows the regression line, which minimizes the sum of the squares of the vertical distances between the points and the line—it is theFigure 4.2 Data simulated from a bivariate normal distribution with correlation 0.5. The regression line, which represents the best prediction of y given x, has half the slope of the principal component line, which goes closest through the cloud of points.familiar least squares line, y = aˆ + ˆbx, with aˆ, ˆb chosen to minimize ni=1 (yi − (aˆ + ˆbxi))2. In this case, aˆ = 0 and ˆb = 0.5; the regression line thus has slope 0.5.When given this sort of scatterplot (without any lines superimposed) and asked to draw the regression line of y on x, students tend to draw the principal component line shown in Figure 4.2a. However, for the goal of predicting y from x, or for estimating the average of y for any given value of x, the regression line is in fact better—even if it does not appear so at first.The superiority of the regression line for estimating the average of y given x can be seen from a careful study of Figure 4.2. For example, consider the points at the extreme left of either graph. They all lie above the principal components line but are roughly half below and half above the regression line. Thus, the principal component line underpredicts y for low values of x. Similarly, a careful study of the right side of each graph shows that the principal component line overpredicts y for high values of x. In contrast, the regression line again gives unbiased predictions, in the sense of going through the average value of y given x.Regression to the meanRecall that when x and y are standardized (that is, placed on a common scale, as in Figure 4.2), the regression line always has slope less than 1. Thus, when x is 1 standard deviations above the mean, the predicted value of y is somewhere between 0 and 1 standard deviations above the mean. This phenomenon in linear models—that y is predicted to be closer to the mean (in standard-deviation units) than x—is called regression to the mean and occurs in many vivid contexts.For example, if a woman is 10 inches taller than the average for her sex, and the correlation of mothers’ and (adult) sons’ heights is 0.5, then her son’s predicted height is 5 inches taller than the average for men. He is expected to be taller than average, but not so much taller—thus a “regression” (in the nonstatistical sense) to the average.A similar calculation can be performed for any pair of variables that are not perfectly correlated. For example, let xi and yi be the number of games won by baseball team i in two successive seasons. They will not be correlated 100%; thus, we would expect the teams that did the best in season 1 (that is, with highest values of x) to do not as well in season 2 (that is, with values of y that are closer to the average for all the teams). Similarly, we would expect a team with a poor record in season 1 to improve in season 2.A naive interpretation of regression to the mean is that heights, or baseball records, or other variable phenomena necessarily become more and more “average” over time. This view is mistaken because it ignores the error in the regression predicting y from x. For any data point xi, the point prediction for its yi will be regressed toward the mean, but the actual yi that is observed will not be exactly where it is predicted. Some points end up falling closer to the mean and some fall further. This can be seen in Figure 4.2b.4.4 Logarithmic transformationsWhen additivity and linearity (see Section 3.6) are not reasonable assumptions, a nonlinear transformation can sometimes remedy the situation. It commonly makes sense to take the logarithm of outcomes that are all-positive. For outcome variables, this becomes clear when we think about making predictions on the original scale. The regression model imposes no constraints that would force these predictions to be positive as well. However, if we take the logarithm of the variable, run the model, make predictions on the log scale, and then transform back (by exponentiating), the resulting predictions are necessarily positive because for any real a, exp(a) > 0.Perhaps more importantly, a linear model on the logarithmic scale corresponds to a multiplicative model on the original scale. Consider the linear regression modellogyi = b0 +b1Xi1 +b2Xi2 +···+i Exponentiating both sides yieldsyi = eb0 +b1 Xi1 +b2 Xi2 +···+i = B0·BXi1·BXi2···Ei12where B0 = eb0 , B1 = eb1 , B2 = eb2 , . . . are exponentiated regression coefficients (and thus are positive), and Ei = ei is the exponentiated error term (also positive). On the scale of the original data yi, the predictors Xi1, Xi2, . . . come in multiplicatively.Height and earnings exampleWe illustrate logarithmic regression by considering models predicting earnings from height. Expression (4.1) on page 53 shows a linear regression of earnings on height. However, it really makes more sense to model earnings on the logarithmic scale (our model here excludes those people who reported zero earnings). We can fit a regression to log earnings and then take the exponential to get predictions on the original scale.Direct interpretation of small coefficients on the log scale.of earnings and regress on height,log.earn <- log (earn)earn.logmodel.1 <- lm (log.earn ~ height) display (earn.logmodel.1)yielding the following estimate:lm(formula = log.earn ~ height) coef.est coef.se (Intercept) 5.74 0.45We take the logarithmR codeR output
Log regression plotted on log scale Log regression plotted on original scaleFigure 4.3 Plot of regression of earnings on height, with solid line showing the fitted log regression model, log(earnings) = 5.78 + 0.06 · height, plotted on the logarithmic and untransformed scales. Compare to the linear model (Figure 4.1a).scatterplot!data and regression lines superimposedFigure 4.4 Interpretation of exponentiated coefficients in a logarithmic regression model as relative difference (curved upper line), and the approximation exp(x) = 1 + x, which is valid for small coefficients x (straight line).height 0.06 0.01n = 1192, k = 2residual sd = 0.89, R-Squared = 0.06The estimated coefficient β1 = 0.06 implies that a difference of 1 inch in height corresponds to an expected positive difference of 0.06 in log(earnings), so that earnings are multiplied by exp(0.06). But exp(0.06) ≈ 1.06 (more precisely, it is 1.062). Thus, a difference of 1 in the predictor corresponds to an expected positive difference of about 6% in the outcome variable. Similarly, if β1 were −0.06, then a positive difference of 1 inch of height would correspond to an expected negative difference of about 6% in earnings.This correspondence does grow weaker as the magnitude of the coefficient increases. Figure 4.4 displays the deterioration of the correspondence as the coefficient size increases. The plot is restricted to coefficients in the range (−1, 1) because, on the log scale, regression coefficients are typically (though not always) less than 1. A coefficient of 1 on the log scale implies that a change of one unit in the predictor is associated with a change of exp(1) = 2.7 in the outcome, and if predictors are parameterized in a reasonable way, it is unusual to see effects of this magnitude.Why we use natural log rather than log-base-10We prefer natural logs (that is, logarithms base e) because, as described above, coefficients on the natural-log scale are directly interpretable as approximate proportional differences: with a coefficient of 0.06, a difference of 1 in x corresponds to an approximate 6% difference in y, and so forth.2Another approach is to take logarithms base 10, which we write as log10. The connection between the two different scales is that log10(x) = log(x)/log(10) = log(x)/2.30. The advantage of log10 is that the predicted values themselves are easier to interpret; for example, when considering the earnings regressions, log10(10,000) = 4 and log10(100,000) = 5, and with some experience we can also quickly read off intermediate values—for example, if log10(earnings) = 4.5, then earnings ≈ 30,000.The disadvantage of log10 is that the resulting coefficients are harder to interpret. For example, if we definelog10.earn <- log10 (earn)the regression on height looks likelm(formula = log10.earn ~ height) coef.est coef.se(Intercept) 2.493 0.197R codeR outputheightn = 1187,residual sd = 0.388, R-Squared = 0.06The coefficient of 0.026 tells us that a difference of 1 inch in height corresponds to a difference of 0.026 in log10(earnings); that is, a multiplicative difference of 100.026 = 1.062. This is the same 6% change as before, but it cannot be seen by simply looking at the coefficient as could be done on the natural-log scale.Building a regression model on the log scaleAdding another predictor. Each inch of height corresponds to a 6% increase in earnings—that seems like a lot! But men are mostly taller than women and also tend to have higher earnings. Perhaps the 6% predictive difference can be “explained” by differences between the sexes. Do taller people earn more, on average, than shorter people of the same sex? We can answer this question by including sex into the regression model—in this case, a predictor called male that equals 1 for men and 0 for women:lm(formula = log.earn ~ height + male) coef.est coef.se0.026 0.003 k = 2(Intercept) heightmalen = 1192, k = residual sd =After controllingdifference of 2%: under this model, two persons of the same sex but differing by 1 inch in height will differ, on average, by 2% in earnings. The predictive comparison of sex, however, is huge: comparing a man and a woman of the same height, the man’s earnings are exp(0.42) = 1.52 times the woman’s; that is, 52% more. (We cannot simply convert the 0.42 to 42% because this coefficient is not so close to zero; see Figure 4.4.)2 Natural log is sometimes written as “ln” or “loge” but we simply write “log” since this is our default.8.15 0.60 0.02 0.01 0.42 0.07 30.88, R-Squared = 0.09for sex, an inch of height corresponds to estimated predictiveR output
R outputNaming inputs. Incidentally, we named this new input variable male so that it could be immediately interpreted. Had we named it sex, for example, we would always have to go back to the coding to check whether 0 and 1 referred to men and women, or vice versa.3Checking statistical significance. The difference between the sexes is huge and well known, but the height comparison is interesting too—a 2% difference, for earnings of $50,000, comes to a nontrivial $1000 per inch. To judge statistical significance, we can check to see if the estimated coefficient is more than 2 standard errors from zero. In this case, with an estimate of 0.02 and standard error of 0.01, we would need to display to three decimal places to be sure (using the digits option in the display() function):lm(formula = log.earn ~ height + male) coef.est coef.seR code R outputn = 1192, k = 3residual sd = 0.88, R-Squared = 0.09The coefficient for height indeed is statistically significant. Another way to check significance is to directly compute the 95% confidence interval based on the inferential simulations, as we discuss in Section 7.2.Residual standard deviation and R2. Finally, the regression model has a residual standard deviation of 0.88, implying that approximately 68% of log earnings will be within 0.88 of the predicted value. On the original scale, approximately 68% of earnings will be within a factor of exp(0.88) = 2.4 of the prediction. For example, a 70-inch person has predicted earnings of 8.153 + 0.021 · 70 = 9.623, with a predictive standard deviation of approximately 0.88. Thus, there is an approximate 68% chance that this person has log earnings in the range [9.623 ± 0.88] = [8.74, 10.50], which corresponds to earnings in the range [exp(8.74), exp(10.50)] = [6000, 36000]. This very wide range tells us that the regression model does not predict earnings well—it is not very impressive to have a prediction that can be wrong by a factor of 2.4—and this is also reflected in the R2, which is only 0.09, indicating that only 9% of the variance in the data is explained by the regression model. This low R2 manifests itself graphically in Figure 4.3, where the range of the regression predictions is clearly much narrower than the range of the data.Including an interaction. We now consider a model with an interaction between height and sex, so that the predictive comparison for height can differ for men and women:earn.logmodel.3 <- lm (log.earn ~ height + male + height:male)(Intercept) heightmale8.153 0.603 0.021 0.009 0.423 0.072which yields(Intercept) heightmale height:malecoef.est 8.388 0.017 -0.079 0.007coef.se 0.844 0.013 1.258 0.019n = 1192, k residual sd= 4= 0.88, R-Squared = 0.093 Another approach would be to consider sex variable as a factor with two named levels, male and female; see page 68. Our point here is that, if the variable is coded numerically, it is convenient to give it the name male corresponding to the coding of 1.

That is,log(earnings) = 8.4 + 0.017 · height − 0.079 · male + 0.007 · height · male. (4.2) We shall interpret each of the four coefficients in this model.• The intercept is the predicted log earnings if height and male both equal zero. Because heights are never close to zero, the intercept has no direct interpretation.• The coefficient for height is the predicted difference in log earnings corresponding to a 1-inch difference in height, if male equals zero. Thus, the estimated predictive difference per inch of height is 1.7% for women. The estimate is less than 2 standard errors from zero, indicating that the data are consistent with a zero or negative predictive difference also.• The coefficient for male is the predicted difference in log earnings between women and men, if height equals 0. Heights are never close to zero, and so the coefficient for male has no direct interpretation in this model. (We have already encountered this problem; for example, consider the difference between the intercepts of the two lines in Figure 3.4b on page 35.)• The coefficient for height:male is the difference in slopes of the lines predicting log earnings on height, comparing men to women. Thus, an inch of height corresponds to 0.7% more of an increase in earnings among men than among women, and the estimated predictive difference per inch of height among men is 1.7% + 0.7% = 2.4%.The interaction coefficient is not statistically significant, but it is plausible that the correlation between height and earnings is stronger for men and women, and so we keep it in the model, following general principles we discuss more fully in Section 4.6.Linear transformation to make coefficients more interpretable. We can make the parameters in the interaction model clearer to interpret by rescaling the height predictor to have a mean of 0 and standard deviation 1:z.height <- (height - mean(height))/sd(height)For these data, mean(height) and sd(height) are 66.9 inches and 3.8 inches, respectively. Fitting the model to z.height, male, and their interaction yieldslm(formula = log.earn ~ z.height + male + z.height:male) coef.est coef.se(Intercept) 9.53 0.05 z.height 0.07 0.05 male 0.42 0.07 z.height:male 0.03 0.07n = 1192, k = 4residual sd = 0.88, R-Squared = 0.09We can now interpret all four of the coefficients:• The intercept is the predicted log earnings if z.height and male both equal zero. Thus, a 66.9-inch tall woman is predicted to have log earnings of 9.53, and thus earnings of exp(9.53) = 14000.• The coefficient for z.height is the predicted difference in log earnings corresponding to a 1 standard-deviation difference in height, if male equals zero. Thus, the estimated predictive difference for a 3.8-inch increase in height is 7% for women.R codeR output
R outputOne might also consider centering the predictor for sex, but here it is easy enough to interpret male = 0, which corresponds to the baseline category (in this case, women).Further difficulties in interpretationFor a glimpse into yet another difficulty in interpreting regression coefficients, consider the simpler log earnings regression without the interaction term. The predictive interpretation of the height coefficient is simple enough: comparing two adults of the same sex, the taller person will be expected to earn 2% more per inch of height (see the model on page 61). This seems to be a reasonable comparison.For the coefficient for sex, we would say: comparing two adults of the same height but different sex, the man will be expected to earn 52% more. But is this a relevant comparison? For example, if we are comparing a 66-inch woman to a 66-inch man, then we are comparing a tall woman to a short man. So, in some sense, they do not differ only in sex. Perhaps a more reasonable comparison would be of an “average woman” to an “average man.”The ultimate solution to this sort of problem must depend on why the model is being fit in the first place. For now we shall focus on the technical issues of fitting reasonable models to data. We return to issues of interpretation in Chapters 9 and 10.Log-log model: transforming the input and outcome variablesIf the log transformation is applied to an input variable as well as the outcome, the coefficient can be interpreted as the expected proportional change in y per proportional change in x. For example:lm(formula = log.earn ~ log.height + male) coef.est coef.se(Intercept) 3.62 2.60 log.height 1.41 0.62 male 0.42 0.07n = 1192, k = 364••The coefficient for male is the predicted difference in log earnings between women and men, if z.height equals 0. Thus, a 66.9-inch man is predicted to have log earnings that are 0.42 higher than that of a 66.9-inch woman. This corresponds to a ratio of exp(0.42) = 1.52, so the man is predicted to have 52% higher earnings than the woman.The coefficient for z.height:male is the difference in slopes between the predictive differences for height among women and men. Thus, a 3.8-inch difference of height corresponds to 3% more of an increase in earnings for men than for women, and the estimated predictive comparison among men is 7% + 3% = 10%.residual sd =0.88, R-Squared = 0.09For each 1% difference in height, the predicted difference in earnings is 1.41%. The other input, male, is categorical so it does not make sense to take its logarithm.In economics, the coefficient in a log-log model is sometimes called an “elasticity”; see Exercise 4.6 for an example.
Taking logarithms even when not necessaryIf a variable has a narrow dynamic range (that is, if the ratio between the high and low values is close to 1), then it will not make much of a difference in fit if the regression is on the logarithmic or the original scale. For example, the standard deviation of log.height in our survey data is 0.06, meaning that heights in the data vary by only approximately a factor of 6%.In such a situation, it might seem to make sense to stay on the original scale for reasons of simplicity. However, the logarithmic transformation can make sense even here, because coefficients are often more easily understood on the log scale. The choice of scale comes down to interpretability: whether it is easier to understand the model as proportional increase in earnings per inch, or per proportional increase in height.For an input with a larger amount of relative variation (for example, heights of children, or weights of animals), it would make sense to work with its logarithm immediately, both as an aid in interpretation and likely an improvement in fit too.4.5 Other transformationsSquare root transformationsThe square root is sometimes useful for compressing high values more mildly than is done by the logarithm. Consider again our height and earnings example.Fitting a linear model to the raw, untransformed scale seemed inappropriate. Expressed in a different way than before, we would expect the differences between people earning nothing versus those earning $10,000 to be far greater than the differences between people earning, say, $80,000 versus $90,000. But under the linear model, these are all equal increments (as in model (4.1)), where an extra inch is worth $1300 more in earnings at all levels.On the other hand, the log transformation seems too severe with these data. With logarithms, the differences between populations earning $5000 versus $10,000 is equivalent to the differences between those earning $40,000 versus those earning $80,000. On the square root scale, however, the differences between the 0 earnings and $10,000 earnings groups are about the same as comparisons between $10,000 and $40,000 or between $40,000 and $90,000. (These move from 0 to 100, 200, and 300 on the square root scale.) See Chapter 25 for more on this example.Unfortunately, models on the square root scale lack the clean interpretation of the original-scale and log-transformed models. For one thing, large negative predictions on this scale get squared and become large positive values on the original scale, thus introducing a nonmonotonicity in the model. We are more likely to use the square root model for prediction than with models whose coefficients we want to understand.Idiosyncratic transformationsSometimes it is useful to develop transformations tailored for specific problems. For example, with the original height-earnings data it would have not been possible to simply take the logarithm of earnings as many observations had zero values. Instead, a model can be constructed in two steps: (1) model the probability that earnings exceed zero (for example, using a logistic regression; see Chapter 5); (2) fit a linear regression, conditional on earnings being positive, which is what we did
Figure 4.5 Histogram of handedness scores of a sample of students. Scores range from −1 (completely left-handed) to +1 (completely right-handed) and are based on the responses to ten questions such as “Which hand do you write with?” and “Which hand do you use to hold a spoon?” The continuous range of responses shows the limitations of treating handedness as a dichotomous variable. From Gelman and Nolan (2002).in the example above. One could also model total income, but economists are often interested in modeling earnings alone.In any case, plots and simulation should definitely be used to summarize inferences, since the coefficients of the two parts of the model combine nonlinearly in their joint prediction of earnings. We discuss this sort of model further in Sections 6.7 and 7.4.What sort of transformed scale would be appropriate for a variable such as “assets” that can be negative, positive, or zero? One possibility is a discrete coding that compresses the high range, for example, 0 for assets in the range [−$100, $100], 1 for assets between $100 and $1000, 2 for assets between $1000 and $10,000, and so forth, and −1 for assets between −$100 and −$10,000, and so forth. Such a mapping could be expressed more fully as a continuous transformation, but for explanatory purposes it can be convenient to use a discrete scale.Using continuous rather than discrete predictorsMany variables that appear binary or discrete can usefully be viewed as continuous. For example, rather than define “handedness” as −1 for left-handers and +1 for right-handers, one can use a standard ten-question handedness scale that gives an essentially continuous scale from −1 to 1 (see Figure 4.5).We avoid discretizing continuous variables (except as a way of simplifying a complicated transformation, as described previously, or to model nonlinearity, as described later). A common mistake is to take a numerical measure and replace it with a binary “pass/fail” score. For example, suppose we tried to predict election winners, rather than continuous votes. Such a model would not work well, as it would discard much of the information in the data (for example, the distinction between a candidate receiving 51% or 65% of the vote). The model would be “wasting its effort” in the hopeless task of predicting the winner in very close cases. Even if our only goal is to predict the winners, we are better off predicting continuous vote shares and then transforming them into predictions about winners, as in our example with congressional elections in Section 7.3.Using discrete rather than continuous predictorsIn some cases, however, it is appropriate to discretize a continuous variable if a simple monotonic or quadratic relation does not seem appropriate. For example, in modeling political preferences, it can make sense to include age with four indicator variables: 18–29, 29–44, 45–64, and 65+, to allow for different sorts of generational patterns. Furthermore, variables that assign numbers to categories that are ordered but for which the gaps between neighboring categories are not always equivalent are often good candidates for discretization.As an example, Chapter 3 described models for children’s test scores given information about their mothers. Another input variable that can be used in these models is maternal employment, which is defined on a four-point ordered scale:• mom.work = 1: mother did not work in first three years of child’s life • mom.work = 2: mother worked in second or third year of child’s life • mom.work = 3: mother worked part-time in first year of child’s life• mom.work = 4: mother worked full-time in first year of child’s life.Fitting a simple model using discrete predictors yieldslm(formula = kid.score ~ as.factor(mom.work), data = kid.iq)R outputcoef.est (Intercept) 82.0 as.factor(mom.work)2 3.8 as.factor(mom.work)3 11.5 as.factor(mom.work)4 5.2coef.se 2.3 3.1 3.6 2.7n = 434, k = 4residual sd = 20.2, R-Squared = 0.02This parameterization of the model allows for different averages for the children of mothers corresponding to each category of maternal employment. The “baseline” category (mom.work = 1) corresponds to children whose mothers do not go back to work at all in the first three years after the child is born; the average test score for these children is estimated by the intercept, 82.0. The average test scores for the children in the other categories is found by adding the corresponding coefficient to this baseline average. This parameterization allows us to see that the children of mothers who work part-time in the first year after the child is born achieve the highest average test scores, 82.0 + 11.5. These families also tend to be the most advantaged in terms of many other sociodemographic characteristics as well, so a causal interpretation is not warranted.Index and indicator variablesIndex variables divide a population into categories. For example:• male = 1 for males and 0 for females• age = 1 for ages 18–29, 2 for ages 30–44, 3 for ages 45–64, 4 for ages 65+ • state = 1 for Alabama, . . ., 50 for Wyoming• county indexes for the 3082 counties in the United States.Indicator variables are 0/1 predictors based on index variables. For example:• sex.1 = 1 for females and 0 otherwisesex.2 = 1 for males and 0 otherwise• age.1 = 1 for ages 18–29 and 0 otherwise age.2 = 1 for ages 30–44 and 0 otherwise age.3 = 1 for ages 45–64 and 0 otherwiseage.4 = 1 for ages 65+ and 0 otherwise
• 50 indicators for state• 3082 indicators for county.As demonstrated in the previous section, including these variables as regression predictors allows for different means for the populations corresponding to each of the categories delineated by the variable.When to use index or indicator variables. When an input has only two levels, we prefer to code it with a single variable and name it appropriately; for example, as discussed earlier with the earnings example, the name male is more descriptive than sex.1 and sex.2.R also allows variables to be included as factors with named levels; for example, sex would have the levels male and female. In this book, however, we restrict ourselves to numerically defined variables, which is convenient for mathematical notation and also when setting up models in Bugs.When an input has multiple levels, we prefer to create an index variable (thus, for example, age, which can take on the levels 1, 2, 3, 4), which can then be given indicators if necessary. As discussed in Chapter 11, multilevel modeling offers a general approach to such categorical predictors.IdentifiabilityA model is said to be nonidentifiable if it contains parameters that cannot be estimated uniquely—or, to put it another way, that have standard errors of infinity. The offending parameters are called nonidentified. The most familiar and important example of nonidentifiability arises from collinearity of regression predictors. A set of predictors is collinear if there is a linear combination of them that equals 0 for all the data.If an index variable takes on J values, then there are J associated indicator variables. A classical regression can include only J−1 of any set of indicators—if all J were included, they would be collinear with the constant term. (You could include a full set of J by excluding the constant term, but then the same problem would arise if you wanted to include a new set of indicators. For example, you could not include both of the sex categories and all four of the age categories. It is simpler just to keep the constant term and all but one of each set of indicators.)For each index variable, the indicator that is excluded from the regression is known as the default, reference, or baseline condition because it is the implied category if all the J−1 indicators are set to zero. The default in R is to set the first level of a factor as the reference condition; other options include using the last level as baseline, selecting the baseline, and constraining the coefficients to sum to zero. There is some discussion in the regression literature on how best to set reference conditions, but we will not worry about it, because in multilevel models we can include all J indicator variables at once.In practice, you will know that a regression is nonidentified because your computer program will give an error or return “NA” for a coefficient estimate (or it will be dropped by the program from the analysis and nothing will be reported except that it has been removed).4.6 Building regression models for predictionA model must be created before it can be fit and checked, and yet we put “model building” near the end of this chapter. Why? It is best to have a theoretical model laid out before any data analyses begin. But in practical data analysis it is usually easiest to start with a simple model and then build in additional complexity, taking care to check for problems along the way.There are typically many reasonable ways in which a model can be constructed. Models may differ depending on the inferential goals or the way the data were collected. Key choices include how the input variables should be combined in creating predictors, and which predictors should be included in the model. In classical regression, these are huge issues, because if you include too many predictors in a model, the parameter estimates become so variable as to be useless. Some of these issues are less important in multilevel regression but they certainly do not disappear completely.This section focuses on the problem of building models for prediction. Building models that can yield causal inferences is a related but separate topic that is addressed in Chapters 9 and 10.General principlesOur general principles for building regression models for prediction are as follows:1. Include all input variables that, for substantive reasons, might be expected to be important in predicting the outcome.2. It is not always necessary to include these inputs as separate predictors—for example, sometimes several inputs can be averaged or summed to create a “total score” that can be used as a single predictor in the model.3. For inputs that have large effects, consider including their interactions as well.4. We suggest the following strategy for decisions regarding whether to exclude a variable from a prediction model based on expected sign and statistical significance (typically measured at the 5% level; that is, a coefficient is “statistically significant” if its estimate is more than 2 standard errors from zero):(a) If a predictor is not statistically significant and has the expected sign, it is generally fine to keep it in. It may not help predictions dramatically but is also probably not hurting them.(b) If a predictor is not statistically significant and does not have the expected sign (for example, incumbency having a negative effect on vote share), consider removing it from the model (that is, setting its coefficient to zero).(c) If a predictor is statistically significant and does not have the expected sign, then think hard if it makes sense. (For example, perhaps this is a country such as India in which incumbents are generally unpopular; see Linden, 2006.) Try to gather data on potential lurking variables and include them in the analysis.(d) If a predictor is statistically significant and has the expected sign, then by all means keep it in the model.These strategies do not completely solve our problems but they help keep us from making mistakes such as discarding important information. They are predicated on having thought hard about these relationships before fitting the model. It’s always easier to justify a coefficient’s sign after the fact than to think hard ahead of time about what we expect. On the other hand, an explanation that is determined after running the model can still be valid. We should be able to adjust our theories in light of new information.
R outputlm(formula = weight ~ diam1 + diam2 +canopy.height + total.height +R outputmindiam1 0.8diam2 0.4canopy.height 0.5 total.height 0.6 density 1.0 group 0.0weight 60q25 median 1.4 2.0 1.0 1.5 0.9 1.1 1.2 1.5 1.0 1.0 0.0 0.0220 360q75 max IQR 2.5 5.2 1.1 1.9 4.0 0.9 1.3 2.5 0.4 1.7 3.0 0.5 2.0 9.0 1.0 1.0 1.0 1.0690 4050 470Example: predicting the yields of mesquite bushesWe illustrate some ideas of model checking with a real-data example that is nonetheless somewhat artificial in being presented in isolation from its applied context. Partly because this example is not a “success story” and our results are inconclusive, it represents the sort of analysis a student might perform in exploring a new dataset.Data were collected in order to develop a method of estimating the total production (biomass) of mesquite leaves using easily measured parameters of the plant, before actual harvesting takes place. Two separate sets of measurements were taken, one on a group of 26 mesquite bushes and the other on a different group of 20 mesquite bushes measured at a different time of year. All the data were obtained in the same geographical location (ranch), but neither constituted a strictly random sample.The outcome variable is the total weight (in grams) of photosynthetic material as derived from actual harvesting of the bush. The input variables are:diam1: diameter of the canopy (the leafy area of the bush)in meters, measured along the longer axis of the bushdiam2: canopy diameter measured along the shorter axiscanopy.height: height of the canopytotal.height: total height of the bushdensity: plant unit density (# of primary stems per plant unit) group: group of measurements (0 for the first group,1 for the second group)It is reasonable to predict the leaf weight using some sort of regression model. Many formulations are possible. The simplest approach is to regress weight on all of the predictors, yielding the estimates:density +(Intercept) diam1diam2 canopy.height total.height densitygroupn = 46, k = residual sdgroup, data = mesquite) coef.est coef.se-729 147 190 113 371 124 356 210-102 186 131 34 -363 1007= 269, R-Squared= 0.85To get a sense of the importance of each predictor, it is useful to know the range of each variable:“IQR” in the last column refers to the interquartile range—the difference between the 75th and 25th percentile points of each variable.
But perhaps it is more reasonable to fit on the logarithmic scale, so that effects are multiplicative rather than additive:lm(formula = log(weight) ~ log(diam1) + log(diam2) + log(canopy.height) + log(total.height) + log(density) + group, data = mesquite)coef.se IQR of predictor 0.17 --0.28 0.6 0.21 0.6 0.28 0.4 0.31 0.40.12 0.30.13 1.0n = 46, k = 7residual sd = 0.33, R-Squared = 0.89Instead of, “each meter difference in canopy height is associated with an additional 356 grams of leaf weight,” we have, “a difference of x% in canopy height is associated with an (approximate) positive difference of 0.37x% in leaf weight” (evaluated at the same levels of all other variables across comparisons).So far we have been throwing all the predictors directly into the model. A more “minimalist” approach is to try to come up with a simple model that makes sense. Thinking geometrically, we can predict leaf weight from the volume of the leaf canopy, which we shall roughly approximate ascanopy.volume = diam1 · diam2 · canopy.height.This model is an oversimplification: the leaves are mostly on the surface of a bush, not in its interior, and so some measure of surface area is perhaps more appropriate. We shall return to this point shortly.It still makes sense to work on the logarithmic scale:lm(formula = log(weight) ~ log(canopy.volume)) coef.est coef.se(Intercept) 5.17 0.08 log(canopy.volume) 0.72 0.05n = 46, k = 2residual sd = 0.41, R-Squared = 0.80Thus, leaf weight is approximately proportional to canopy.volume to the 0.72 power. It is perhaps surprising that this power is not closer to 1. The usual explanation for this is that there is variation in canopy.volume that is unrelated to the weight of the leaves, and this tends to attenuate the regression coefficient—that is, to decrease its absolute value from the “natural” value of 1 to something lower. Similarly, regressions of “after” versus “before” typically have slopes of less than 1. (For another example, Section 7.3 has an example of forecasting congressional elections in which the vote in the previous election has a coefficient of only 0.58.)The regression with only canopy.volume is satisfyingly simple, with an impressive R-squared of 80%. However, the predictions are still much worse than the model with all the predictors. Perhaps we should go back and put in the other predictors. We shall define:canopy.area = diam1 · diam2 canopy.shape = diam1/diam2.The set (canopy.volume, canopy.area, canopy.shape) is then just a different parameterization of the three canopy dimensions. Including them all in the model yields:R outputcoef.est 5.35 0.39 1.15 0.37 0.39 0.11 group -0.58(Intercept) log(diam1) log(diam2) log(canopy.height) log(total.height) log(density)R output
R outputlm(formula = log(weight) ~ log(canopy.volume) + log(canopy.area) + log(canopy.shape) + log(total.height) + log(density) + group)coef.est coef.se (Intercept) 5.35 0.17 log(canopy.volume) 0.37 0.28 log(canopy.area) 0.40 0.29 log(canopy.shape) -0.38 0.23 log(total.height) 0.39 0.31 log(density) 0.11 0.12 group -0.58 0.13n = 46, k = 7residual sd = 0.33, R-Squared = 0.89This fit is identical to that of the earlier log-scale model (just a linear transformation of the predictors), but to us these coefficient estimates are more directly interpretable:• Canopy volume and area are both positively associated with weight. Neither is statistically significant, but we keep them in because they both make sense: (1) a larger-volume canopy should have more leaves, and (2) conditional on volume, a canopy with larger cross-sectional area should have more exposure to the sun.• The negative coefficient of canopy.shape implies that bushes that are more circular in cross section have more leaf weight (after controlling for volume and area). It is not clear whether we should “believe” this. The coefficient is not statistically significant; we could keep this predictor in the model or leave it out.• Total height is positively associated with weight, which could make sense if the bushes are planted close together—taller bushes get more sun. The coefficient is not statistically significant, but it seems to make sense to “believe” it and leave it in.• It is not clear how to interpret the coefficient for density. Since it is not statistically significant, maybe we can exclude it.• For whatever reason, the coefficient for group is large and statistically significant, so we must keep it in. It would be a good idea to learn how the two groups differ so that a more relevant measurement could be included for which group is a proxy.This leaves us with a model such aslm(formula = log(weight) ~ log(canopy.volume) + log(canopy.area) + group)coef.est coef.se (Intercept) 5.22 0.09 log(canopy.volume) 0.61 0.19 log(canopy.area) 0.29 0.24 group -0.53 0.12n = 46, k = 4residual sd = 0.34, R-Squared = 0.87orR outputR outputlm(formula = log(weight) ~ log(canopy.volume) + log(canopy.area) + log(canopy.shape) + log(total.height) + group)coef.est coef.se (Intercept) 5.31 0.16 log(canopy.volume) 0.38 0.28log(canopy.area) log(canopy.shape) log(total.height) group0.41 0.29 -0.32 0.22 0.42 0.31 -0.54 0.12n = 46, k = 6residual sd = 0.33, R-Squared = 0.88We want to include both volume and area in the model, since for geometrical reasons we expect both to be positively predictive of leaf volume. It would also make sense to look at some residual plots to look for any patterns in the data beyond what has been fitted by the model.Finally, it would seem like a good idea to include interactions of group with the other predictors. Unfortunately, with only 46 data points, it turns out to be impossible to estimate these interactions accurately: none of them are statistically significant.To conclude this example: we have had some success in transforming the outcome and input variables to obtain a reasonable predictive model. However, we do not have any clean way of choosing among the models (or combining them). We also do not have any easy way of choosing between the linear and log-transformation models, or bridging the gap between them. For this problem, the log model seems to make much more sense, but we would also like a data-based reason to prefer it, if it is indeed preferable.4.7 Fitting a series of regressionsIt is common to fit a regression model repeatedly, either for different datasets or to subsets of an existing dataset. For example, one could estimate the relation between height and earnings using surveys from several years, or from several countries, or within different regions or states within the United States.As discussed in Part 2 of this book, multilevel modeling is a way to estimate a regression repeatedly, partially pooling information from the different fits. Here we consider the more informal procedure of estimating the regression separately— with no pooling between years or groups—and then displaying all these estimates together, which can be considered as an informal precursor to multilevel modeling.4Predicting party identificationPolitical scientists have long been interested in party identification and its changes over time. We illustrate here with a series of cross-sectional regressions modeling party identification given political ideology and demographic variables.We use the National Election Study, which asks about party identification on a 1– 7 scale (1 = strong Democrat, 2 = Democrat, 3 = weak Democrat, 4 = independent, . . . , 7 = strong Republican), which we treat as a continuous variable. We include the following predictors: political ideology (1 = strong liberal, 2 = liberal, . . . , 7 = strong conservative), ethnicity (0=white, 1=black, 0.5=other), age (as categories: 18–29, 30–44, 45–64, and 65+ years, with the lowest age category as a baseline), education (1 = no high school, 2 = high school graduate, 3 = some college, 4 =4 The method of repeated modeling, followed by time-series plots of estimates, is sometimes called the “secret weapon” because it is so easy and powerful but yet is rarely used as a data-analytic tool. We suspect that one reason for its rarity of use is that, once one acknowledges the time-series structure of a dataset, it is natural to want to take the next step and model that directly. In practice, however, there is a broad range of problems for which a cross-sectional analysis is informative, and for which a time-series display is appropriate to give a sense of trends.
Intercept Ideology Black Age.30.44 Age.45.64Age.65.up Education Female IncomeFigure 4.6 Estimated coefficients (and 50% intervals) for the regression of party identification on political ideology, ethnicity, and other predictors, as fit separately to poll data from each presidential election campaign from 1976 through 2000. The plots are on different scales, with the input variables ordered roughly in declining order of the magnitudes of their coefficients. The set of plots illustrates the display of inferences from a series of regressions.college graduate), sex (0=male, 1=female), and income (1=0–16th percentile, 2= 17–33rd percentile, 3=34–67th percentile, 4=68–95th percentile, 5=96–100th percentile).Figure 4.6 shows the estimated coefficients tracked over time. Ideology and ethnicity are the most important,5 and they remain fairly stable over time. The predictive differences for age and sex change fairly dramatically during the thirty-year period.4.8 Bibliographic noteFor additional reading on transformations, see Atkinson (1985), Mosteller and Tukey (1977), Box and Cox (1964), and Carroll and Ruppert (1981). Bring (1994) has a thorough discussion on standardizing regression coefficients; see also Blalock (1961) and Greenland, Schlessman, and Criqui (1986). Harrell (2001) discusses strategies for regression modeling.For more on the earnings and height example, see Persico, Postlewaite, and Silverman (2004) and Gelman and Nolan (2002). For more on the handedness example, see Gelman and Nolan (2002, sections 2.5 and 3.3.2). The historical background of regression to the mean is covered by Stigler (1986), and its connections to multilevel modeling are discussed by Stigler (1983).The mesquite bushes example in Section 4.6 comes from an exam problem from the 1980s; we have not been able to track down the original data. For more on the ideology example in Section 4.7, see Bafumi (2005).4.9 Exercises1. Logarithmic transformation and regression: consider the following regression: log(weight) = −3.5 + 2.0 log(height) + error,5 Ideology is on a seven-point scale, so that its coefficients must be multiplied by 4 to get the expected change when comparing a liberal (ideology=2) to a conservative (ideology=6).with errors that have standard deviation 0.25. Weights are in pounds and heights are in inches.(a) Fill in the blanks: approximately 68% of the persons will have weights within a factor of  and  of their predicted values from the regression.(b) Draw the regression line and scatterplot of log(weight) versus log(height) that make sense and are consistent with the fitted model. Be sure to label the axes of your graph.2. The folder earnings has data from the Work, Family, and Well-Being Survey (Ross, 1990). Pull out the data on earnings, sex, height, and weight.(a) In R, check the dataset and clean any unusually coded data.(b) Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this modelas average earnings for people with average height?(c) Fit some regression models with the goal of predicting earnings from somecombination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.(d) Interpret all model coefficients.3. Plotting linear and nonlinear regressions: we downloaded data with weight (in pounds) and age (in years) from a random sample of American adults. We first created new variables: age10 = age/10 and age10.sq = (age/10)2, and indicators age18.29, age30.44, age45.64, and age65up for four age categories. We then fit some regressions, with the following results:lm(formula = weight ~ age10) coef.est coef.se (Intercept) 161.0 7.3 age10 2.6 1.6n = 2009, k = 2residual sd = 119.7, R-Squared = 0.00lm(formula = weight ~ age10 + age10.sq) coef.est coef.seR output(Intercept) age10 age10.sqn = 2009, k = residual sd =96.2 19.3 33.6 8.7 -3.2 0.9 3119.3, R-Squared = 0.01lm(formula =(Intercept) age30.44TRUE age45.64TRUE age65upTRUEweight ~ coef.est 157.2 19.1 27.2 8.5age30.44 + age45.64 + age65up) coef.se5.4 7.0 7.6 8.7n = 2009, k = residual sd =4119.4, R-Squared = 0.01(a) On a graph of weights versus age (that is, weight on y-axis, age on x-axis), draw the fitted regression line from the first model.(b) On the same graph, draw the fitted regression line from the second model.(c) On another graph with the same axes and scale, draw the fitted regression line from the third model. (It will be discontinuous.)4. Logarithmic transformations: the folder pollution contains mortality rates and various environmental factors from 60 U.S. metropolitan areas (see McDonald and Schwing, 1973). For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.(a) Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.(b) Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.(c) Interpret the slope coefficient from the model you chose in (b).(d) Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations whenhelpful. Plot the fitted regression model and interpret the coefficients.(e) Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in (d), so this is not really cross-validation, but it gives a sense of how thesteps of cross-validation can be implemented.)5. Special-purpose transformations: for a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values Di and Ri. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.(a) Discuss the advantages and disadvantages of the following measures:• The simple difference, Di − Ri• The ratio, Di/Ri• The difference on the logarithmic scale, log Di − log Ri • The relative proportion, Di/(Di + Ri).(b) Propose an idiosyncratic transformation (as in the example on page 65) and discuss the advantages and disadvantages of using it as a regression input.6. An economist runs a regression examining the relations between the average price of cigarettes, P , and the quantity purchased, Q, across a large sample of counties in the United States, assuming the following functional form, log Q = α+β log P . Suppose the estimate for β is 0.3. Interpret this coefficient.7. Sequence of regressions: find a regression problem that is of interest to you and can be performed repeatedly (for example, data from several years, or for several countries). Perform a separate analysis for each year, or country, and display the estimates in a plot as in Figure 4.6 on page 74.8. Return to the teaching evaluations data from Exercise 3.5. Fit regression models predicting evaluations given many of the inputs in the dataset. Consider interactions, combinations of predictors, and transformations, as appropriate. Consider several models, discuss in detail the final model that you choose, and also explain why you chose it rather than the others you had considered.
