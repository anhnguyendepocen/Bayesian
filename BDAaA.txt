Appendix AStandard probability distributionsTables A.1 and A.2 present notation, probability density functions, parameter descriptions, means, modes, and standard deviations for several standard probability distributions. We use the standard notation θ for the random variable (or random vector), except in the case of the Wishart and inverse-Wishart, for which we use W for the random matrix, and LKJ correlation, for which we use Σ for a correlation matrix.Realistic distributions for complicated multivariate models, including hierarchical and mixture models, can typically be constructed using, as building blocks, the simple distribu- tions listed here. In our own work we use preprogrammed random number routines (many available in R, for example), but it can be valuable to understand where these numbers come from.The starting point for any simulations are functions that draw pseudorandom samples from the uniform distribution on the unit interval. Much research has been done to ensure that the pseudorandom numbers are appropriate for realistic applied tasks. For example, a sequence may appear uniform in one dimension while m-tuples are not randomly scattered in m dimensions.A.1 Continuous distributionsUniformThe uniform distribution is used to represent a variable that is known to lie in an interval and equally likely to be found anywhere in the interval. A noninformative distribution is obtained in the limit as a → −∞, b → ∞. If u is drawn from a standard uniform distribution U(0, 1), then θ = a + (b − a)u is a draw from U(a, b).Univariate normalThe normal, or Gaussian, distribution is ubiquitous in statistics. Sample averages are approximately normally distributed by the central limit theorem. A noninformative or flat distribution is obtained in the limit as the variance σ → ∞. The variance is usually restricted to be positive; σ = 0 corresponds to a point mass at θ. The density function is always finite, the integral is finite as long as σ is finite. If z is a random deviate from the standard normal distribution, then θ = μ + σz is a draw from N(μ, σ2).Two properties of the normal distribution that play a large role in model building and Bayesian computation are the addition and mixture properties.The sum of two independent normal random variables is normally distributed. If θ1 and θ2 are independent with N(μ1, σ12) and N(μ2, σ2) distributions, then θ1 + θ2 ∼ N(μ1 + μ2,σ12 +σ2). The mixture property states that if θ1|θ2 ∼ N(θ2,σ12) and θ2 ∼ N(μ2,σ2), then θ1 ∼ N(μ2, σ12 + σ2). This is useful in the analysis of hierarchical normal models.575
576STANDARD PROBABILITY DISTRIBUTIONSTable A.1Distribution UniformNormalLognormalMultivariate normalGammaInverse-gammaChi-squareInverse-chi-squareScaled inverse-chi-squareExponentialLaplace (double-exponential)WeibullWishartInverse-WishartLKJ correlationContinuous distributionsNotationθ ∼ U(α, β)p(θ) = U(θ|α, β) θ ∼ N(μ,σ2)p(θ) = N(θ|μ, σ2)θ ∼ lognormal(μ, σ2)p(θ) = lognormal(θ|μ, σ2) θ ∼ N(μ, Σ)p(θ) = N(θ|μ, Σ) (implicit dimension d)θ ∼ Gamma(α, β)p(θ) = Gamma(θ|α, β)θ ∼ Inv-gamma(α, β)p(θ) = Inv-gamma(θ|α, β)θ∼χ2ν 2 p(θ) = χν (θ)θ∼Inv-χ2ν 2 p(θ) = Inv-χν (θ)θ ∼ Inv-χ2(ν, s2)p(θ) = Inv-χ2(θ|ν, s2)θ ∼ Expon(β) p(θ) = Expon(θ|β)θ ∼ Laplace(μ, σ)p(θ) = Laplace(θ|μ, σ)θ ∼ Weibull(α, β)p(θ) = Weibull(θ|α, β)W ∼ Wishartν (S)p(W ) = Wishartν (W |S) (implicit dimension k × k)W ∼ Inv-Wishartν(S−1)p(W) = Inv-Wishartν(W|S−1) (implicit dimension k × k)Σ ∼ LkjCorr(η)p(Σ) = LkjCorr(Σ|η) (implicit dimension k × k)Parameters boundaries α, βwithβ>α location μscaleσ>0 location μlog-scale σ > 0 symmetric, pos. definite,d × d variance matrix Σ shape α > 0inverse scale β > 0 shape α > 0scale β > 0degrees of freedom ν >0degrees of freedom ν >0 degrees of freedom ν >0scales>0inverse scale β > 0location μ scale σ > 0shape α > 0 scale β > 0degrees of freedom ν symmetric, pos. definitek × k scale matrix Sdegrees of freedom ν symmetric, pos. definitek × k scale matrix S shape η > 0
CONTINUOUS DISTRIBUTIONS Density functionp(θ)= 1 , θ∈[α,β] β−α577Mean, variance, and mode E(θ) = α+β22 var(θ)=(β−α)12p(θ)=√1 2πσexp − 12(θ−μ)2 2σ􏰞􏰀no mode E(θ)=μvar(θ) = σ2 mode(θ) = μE(θ) = exp(μ + 1 σ2), √−112 222p(θ)=( 2πσθ) exp(−2σ2(logθ−μ) ) var(θ)=exp(2μ+σ )(exp(σ )−1) mode(θ) = exp(μ − σ2)p(θ) = (2π)−d/2|Σ|−1/2× exp 􏰞− 1 (θ − μ)T Σ−1 (θ − μ)􏰀E(θ) = μ var(θ) = Σ mode(θ) = μ2βα p(θ)=Γ(α)θα−1 −βθ eE(θ) = α βααα−1 2var(θ)= β ,α>2β Γ(α)θ−(α+1)e−β/θ,p(θ) = 2−ν/2 θν/2−1e−θ/2, θ>0same as Gamma(α = 2,β = 2) p(θ) = 2−ν/2 θ−(ν/2+1)e−1/(2θ), θ > 0Γ(ν/2) ν 1 same as Inv-gamma(α = 2,β = 2)same as Gamma(α = 1, β)p(θ) =θ>0 Γ(ν/2) ν 1(α−1)2 (α−2) mode(θ) = β, θ>0var(θ)=βmode(θ)= α−1, forα≥12βE(θ)= β ,forα>1α+1E(θ)=νvar(θ) = 2ν mode(θ)=ν−2,forν≥21E(θ) = , for ν > 2 ν−2 2var(θ) = (ν−2)2(ν−4),ν>4 mode(θ) = 1ν/2 Γ(ν /2)ν+2 E(θ) = ν s22(ν −2)2 (ν −4)sνθ−(ν/2+1)e−νs2/(2θ), θ>0 same as Inv-gamma(α = ν ,β = ν s2)ν−2p(θ) = (ν/2)p(θ) = βe−βθ, θ>02ν mode(θ) = ν s2s4var(θ) =22ν+2E(θ) = 1 β1E(θ)=μvar(θ) = 2σ2mode(θ) = μvar(θ)=β2 mode(θ) = 0􏰃􏰄1 exp −|x−μ| 2σ σp(θ) =p(θ)=βαθ exp(−(θ/β)),θ>0E(θ)=βΓ(1+ 1)2α2 12αE(W) = νS−1 E(W)=(ν−k−1) SE(Σ) = Ik,α α−1p(W) = 􏰃2νk/2πk(k−1)/4 􏰑k Γ􏰞ν+1−i􏰀􏰄−1αvar(θ)=β[Γ(1+α)−(Γ(1+α))] mode(θ) = β(1 − 1 )1/αi=1 2 ×exp􏰞−1tr(S−1W)􏰀, W pos. definite×|S|−ν/2|W |(ν−k−1)/2􏰃 2 􏰑k 􏰞ν+1−i􏰀􏰄−1p(W ) = 2νk/2πk(k−1)/4 i=1 Γ 2 ×|S|ν/2|W |−(ν+k+1)/2􏰌􏰞 1 −1 􏰀×exp −2tr(SW ) , W pos. definitep(Σ) = det(Σ)η−1×2k (2η−2+k−i)(k−i) i=1×􏰑k 􏰞B􏰞i+1, i+1􏰀􏰀k i=1 2 2
578STANDARD PROBABILITY DISTRIBUTIONSTable A.1DistributiontMultivariate tBetaDirichletLogisticLog-logisticTable A.2Distribution PoissonBinomialMultinomialNegative binomialBeta- binomialContinuous distributions continued Notationθ ∼ tν(μ,σ2)p(θ) = tν(θ|μ,σ2)tν is short for tν(0,1)θ ∼ tν(μ,Σ)p(θ) = tν (θ|μ, Σ) (implicit dimension d)θ ∼ Beta(α, β)p(θ) = Beta(θ|α, β)θ ∼ Dirichlet(α1 , . . , αk )p(θ) = Dirichlet(θ|α1, . . , αk)θ ∼ Logistic(μ, σ)p(θ) = Logistic(θ|μ, σ)θ ∼ Log-logistic(α, β)p(θ) = Log-logistic(θ|α, β)Discrete distributionsNotationθ ∼ Poisson(λ)p(θ) = Poisson(θ|λ) θ ∼ Bin(n, p)p(θ) = Bin(θ|n, p) θ∼Multin(n;p1,..,pk)p(θ) = Multin(θ|n;p1,..,pk) θ ∼ Neg-bin(α, β)p(θ) = Neg-bin(θ|α, β) θ ∼ Beta-bin(n, α, β)p(θ) = Beta-bin(θ|n, α, β)Parametersdegrees of freedom ν > 0 location μscale σ > 0degrees of freedom ν > 0 location μ = (μ1,..,μd) symmetric, pos. definited × d scale matrix Σ ‘prior sample sizes’α > 0, β > 0 ‘prior sample size􏰌s’αj>0;α0≡ kj=1αj location μscale σ > 0 scale α > 0shape β > 0Parameters‘rate’ λ > 0‘sample size’n (positive integer)‘probability’ p ∈ [0, 1] ‘sample size’􏰌n (positive integer) ‘probabilities’ pj ∈[0,1];kj = 1 p j = 1 shape α > 0inverse scale β > 0‘sample size’n (positive integer)‘prior sample sizes’ α > 0, β > 0
CONTINUOUS DISTRIBUTIONS Density functionΓ((ν+1)/2) 1 θ−μ 2 −(ν+1)/2 p(θ)=Γ(ν/2)√νπσ(1+ν( σ ))p(θ) = Γ((ν+d)/2) |Σ|−1/2 Γ(ν/2)νd/2πd/2×(1 + 1 (θ − μ)T Σ−1(θ − μ))−(ν+d)/2 νp(θ)= Γ(α+β) θα−1(1−θ)β−1 Γ(α)Γ(β)θ ∈ [0, 1]p(θ) = Γ(α1+···+αk)θα1−1 ···θαk−1579 Mean, variance, and modeE(θ)=μ, forν>1 ν 2var(θ)=ν−2σ,forν>2 mode(θ) = μE(θ) = μ, for ν >1 var(θ)= ν Σ,forν>2ν−2 mode(θ) = μE(θ)= αα+β αβvar(θ) = (α+β)2(α+β+1) mode(θ) = α−1E(θj) = αj α0var(θj) = αj(α0−αj)α+β−2Γ(α1 )···Γ(αk ) 1 k 􏰌kα2 (α0 +1)0 αiαjθ1,..,θk ≥ 0; j=1 θj = 1cov(θi,θj) = −α20(α0+1) mode(θj)= αj−1α0 −k var(θ)= 1σ2π2exp(− x−μ ) σσ(1+exp(− x−μ )) σβ x β−1 p(θ)=􏰁α(α)􏰂,θ>0E(θ) = μ 3p(θ)=mode(θ) = μ E(θ) = 11+( x )−β αvar(θ)=α2 2π/β ,β>21+( x )β 2 αsin(2π/β) 􏰃􏰄mode(θ)=α β−1 β , β>1 β+1Mean, variance, and modeE(θ) = λ, var(θ) = λ mode(θ) = ⌊λ⌋E(θ) = npvar(θ) = np(1 − p) mode(θ) = ⌊(n + 1)p⌋E(θj) = npj var(θj)=npj(1−pj) cov(θi,θj) = −npipj1Density function p(θ) = 1 λθ exp(−λ)θ!θ = 0,1,2,...p(θ) = 􏰞nθ􏰀pθ(1 − p)n−θ θ = 0,1,2,...,np(θ) = 􏰞 n 􏰀pθ1 ···pθk θ1 θ2···θk 1􏰌 kθj = 0,1,2,...,n; kj=1 θj = n􏰞θ+α−1􏰀􏰃β 􏰄α􏰃1 􏰄θ p(θ)= α−1 β+1 β+1θ = 0, 1, 2, . . .p(θ)= Γ(n+1) Γ(α+θ)Γ(n+β−θ)E(θ)=α βαvar(θ) = β2 (β + 1) E(θ)=n αΓ(θ+1)Γ(n−θ+1) Γ(α+β+n) × Γ(α+β) , θ = 0,1,2,...,nα+βvar(θ) = n αβ(α+β+n)2Γ(α)Γ(β)(α+β) (α+β+1)
580 STANDARD PROBABILITY DISTRIBUTIONSLognormalIf θ is a random variable that is restricted to be positive, and log θ ∼ N(μ, σ2), then θ is saidto have a lognormal distribution. Using the Jacobian of the log transformation, one candirectly determine that the density is p(θ) = (√2πσθ)−1 exp(− 12 (log θ − μ)2), the mean 12 222σ 2is exp(μ + 2 σ ), the variance is exp(2μ) exp(σ )(exp(σ ) − 1), and the mode is exp(μ − σ ). The geometric mean and geometric standard deviation of a lognormally distributed random variable θ are simply eμ and eσ.Multivariate normalThe multivariate normal density is always finite; the integral is finite as long as det(Σ−1) > 0. A noninformative distribution is obtained in the limit as det(Σ−1) → 0; this limit is not uniquely defined. A random draw from a multivariate normal distribution can be obtained using the Cholesky decomposition of Σ and a vector of univariate normal draws. The Cholesky decomposition of Σ produces a lower-triangular matrix A (the ‘Cholesky factor’) for which AAT = Σ. If z = (z1, . . . , zd) are d independent standard normal random variables, then θ = μ + Az is a random draw from the multivariate normal distribution with covariance matrix Σ.The marginal distribution of any subset of components (for example, θi or (θi,θj)) is also normal. Any linear transformation of θ, such as the projection of θ onto a linear subspace, is also normal, with dimension equal to the rank of the transformation. The conditional distribution of θ, constrained to lie on any linear subspace, is also normal. The addition property holds: if θ1 and θ2 are independent with N(μ1,Σ1) and N(μ2,Σ2) distributions, then θ1 + θ2 ∼ N(μ1 + μ2,Σ1 + Σ2) as long as θ1 and θ2 have the same dimension. We discuss the generalization of the mixture property shortly.The conditional distribution of any subvector of θ given the remaining elements is once again multivariate normal. If we partition θ into subvectors θ = (U, V ), then p(U |V ) is (multivariate) normal:E(U|V ) = E(U) + cov(U, V )var(V )−1(V − E(V )),var(U|V) = var(U)−cov(U,V)var(V)−1cov(V,U), (A.1)where cov(V,U) is a rectangular matrix (submatrix of Σ) of the appropriate dimensions, and cov(U,V)=cov(V,U)T. Inparticular,ifwedefinethematrixofconditionalcoefficients,C = I − [diag(Σ−1)]−1Σ−1,then 􏰍 −1 −1(θi|θj,allj̸=i)∼N(μi+ cij(θj−μj),[(Σ )ii] ). j ̸=iConversely, if we parameterize the distribution of U and V hierarchically: U|V ∼ N(XV,ΣU|V ), V ∼ N(μV ,ΣV ),then the joint distribution of θ is the multivariate normal,θ=􏰅 U 􏰆∼N􏰅􏰅 XμV 􏰆,􏰅 XΣVXT +ΣU|V XΣV 􏰆􏰆.V μV ΣVXT ΣV(A.2)This generalizes the mixture property of univariate normals.The ‘weighted sum of squares,’ SS = (θ − μ)T Σ−1(θ − μ), has a χ2d distribution. Forany matrix A for which AAT = Σ, the conditional distribution of A−1(θ − μ), given SS, is uniform on a (d−1)-dimensional sphere.
CONTINUOUS DISTRIBUTIONS 581GammaThe gamma distribution is the conjugate prior distribution for the inverse of the normal variance and for the mean parameter of the Poisson distribution. The gamma integral is finite if α > 0; the density function is finite if α ≥ 1. Many computer packages generate gamma random variables directly; otherwise, it is possible to obtain draws from a gamma random variable using draws from a uniform as input. The most effective method depends on the parameter α; see the references for details.There is an addition property for independent gamma random variables with the same in- verse scale parameter. If θ1 and θ2 are independent with Gamma(α1, β) and Gamma(α2, β) distributions, then θ1 + θ2 ∼ Gamma(α1 + α2, β). The logarithm of a gamma random variable is approximately normal; raising a gamma random variable to the one-third power provides an even better normal approximation.Inverse-gammaIf θ−1 has a gamma distribution with parameters α,β, then θ has the inverse-gamma dis- tribution. The density is finite always; its integral is finite if α > 0. The inverse-gamma is the conjugate prior distribution for the normal variance. A noninformative distribution is obtained as α, β → 0.Chi-squareThe χ2 distribution is a special case of the gamma distribution, with α = ν/2 and β = 1 . 2The addition property holds since the inverse scale parameter is fixed: if θ1 and θ2 are independent with χ2ν1 and χ2ν2 distributions, then θ1 + θ2 ∼ χ2ν1 +ν2 .Inverse chi-squareThe inverse-χ2 is a special case of the inverse-gamma distribution, with α = ν/2 and β = 1 . 2We also define the scaled inverse chi-square distribution, which is useful for variance param- eters in normal models. To obtain a simulation draw θ from the Inv-χ2(ν,s2) distribution, first draw X from the χ2ν distribution and then let θ = νs2/X.ExponentialThe exponential distribution is the distribution of waiting times for the next event in a Poisson process and is a special case of the gamma distribution with α = 1. Simulation of draws from the exponential distribution is straightforward. If U is a draw from the uniform distribution on [0,1], then −log(U)/β is a draw from the exponential distribution with parameter β.WeibullIf θ is a random variable that is restricted to be positive, and (θ/β)α has an Expon(1)distribution, then θ is said to have a Weibull distribution with shape parameter α > 0and scale parameter β > 0. The Weibull is often used to model failure times in reliabilityanalysis. Using the Jacobian of the log transformation, one can directly determine that thedensity is p(θ) = α θα−1 exp(−(θ/β)α), the mean is βΓ(1 + 1 ), the variance is β2[Γ(1 + βα α2)−(Γ(1+ 1))2], and the mode is β(1− 1)1/α. ααα
582 STANDARD PROBABILITY DISTRIBUTIONSWishartThe Wishart is the conjugate prior distribution for the inverse covariance matrix in a mul- tivariate normal distribution. It is a multivariate generalization of the gamma distribution. The integral is finite if the degrees of freedom parameter, ν, is greater than or equal to the dimension, k. The density is finite if ν ≥ k + 1. A noninformative distribution is ob- tained as ν → 0. The sample covariance matrix for independent and identically distributed multivariate normal data has a Wishart distribution. In fact, multivariate normal simula- tions can be used to simulate a draw from the Wishart distribution, as follows. Simulate α1, . . . , αν , ν􏰌independent samples from a k-dimensional multivariate N(0, S) distribution, then let θ = νi=1 αiαiT . This only works when the distribution is proper; that is, ν ≥ k.Inverse-WishartIf W −1 ∼ Wishartν (S) then W has the inverse-Wishart distribution. The inverse-Wishart is the conjugate prior distribution for the multivariate normal covariance matrix. The inverse- Wishart density is always finite, and the integral is always finite. A degenerate form occurs when ν < k.LKJ correlationThe LKJ distribution (Lewandowski, Kurowicka, and Joe, 2009) is a distribution over positive-definite symmetric matrices with unit diagonals—that is, correlation matrices. If Σ is a correlation matrix, LkjCorr(Σ|η) ∝ det(Σ)η−1, for with the parameter η required to be positive. The shape parameter η can be interpreted like the shape parameter of a symmetric beta distribution. If η = 1, then the density is uniform over all correlation matrices of a given order. If η > 1, the modal correlation matrix is the identity, with the distribution being more concentrated about this mode as η becomes large. For 0 < η < 1, the density has a trough at the identity matrix.tThe t (or Student-t) is the marginal posterior for the normal mean with unknown variance and conjugate prior and can be interpreted as a mixture of normals with common mean and variances that follow an inverse-gamma distribution. The t is also the ratio of a normal random variable and the square root of an independent gamma random variabl􏰘e. To simu- late t, simulate z from a standard normal and x from a χ2ν, then let θ = μ + σz ν/x. The t density is always finite; the integral is finite if ν > 0 and σ is finite. In the limit ν → ∞, the t distribution approaches N(μ, σ2). The case of ν = 1 is called the Cauchy distribution. The t distribution can be used in place of a normal in a robust analysis.To draw from the multivariate tν(μ􏰘,Σ) distribution, generate a vector z ∼ N(0,I) and ascalarx∼χ2ν,thencomputeμ+Az ν/x,whereAsatisfiesAAT =Σ.BetaThe beta is the conjugate prior distribution for the binomial probability. The density is finiteifα,β≥1,andtheintegralisfiniteifα,β>0. Thechoiceα=β=1givesthe standard uniform distribution; α = β = 0.5 and α = β = 0 are also sometimes used as noninformative densities. To simulate θ from the beta distribution, first simulate xα and xβ from χ2α and χ2 distributions, respectively, then let θ = xα .2β xα +xβIt is sometimes useful to estimate quickly the parameters of the beta distribution using
DISCRETE DISTRIBUTIONS the method of moments:583α+β = α = (α + β)E(θ),E(θ)(1−E(θ)) −1 var(θ)β = (α + β)(1 − E(θ)). (A.3) The kth order statistic from a sample of n independent U(0, 1) variables has the Beta(k, n −k + 1) distribution. DirichletThe Dirichlet is the conjugate prior distribution for the parameters of the multinomial distribution. The Dirichlet is a multivariate generalization of the beta distribution. As with the beta, the integral is finite if all of the α’s are positive, and the density is finite if all are greater than or equal to one.The marginal distribution of a single θj is Beta(αj , α0 − αj ). The marginal distribution of a subvector􏰌of θ is Dirichlet; for example (θi,θj,1−θi−θj) ∼ Dirichlet(αi,αj,α0−αi−αj). The conditional distribution of a subvector given the remaining elements is Dirichlet under the condition kj=1 θj = 1.There are two standard approaches to sampling from a Dirichlet distribution. The fastest method generalizes the method used to sample from the beta distribution: draw x1, . . . , xk from independent gamma distr􏰌ibutions with common scale and shape parameters α1, . . . , αk, and for each j, let θj = xj/ ki=1 xi. A less efficient algorithm relies on the univariate marginal and con􏰌ditional distributions being beta and proceeds as follows. Simulate θ1 from a Beta(α1, ki=2 αi) distribution. Then simu􏰌late θ2, . . . , θk−1 in order, as follows. For j = 2,...,k − 1, simulate φj from a Beta(αj, ki=j+1 αi) distribution, and let θj =(1−􏰌j−1θi)φj. Finally,setθk =1−􏰌k−1θi. i=1 i=1Constrained distributionsWe sometimes use notation such as N+ to convey the normal distribution constrained to be positive; that is, the truncated normal distribution. We also have occasion to use the half-t distribution, which is the right half of the t distribution.A.2 Discrete distributionsPoissonThe Poisson distribution is commonly used to represent count data, such as the num- ber of arrivals in a fixed time period. The Poisson distribution has an addition prop- erty: if θ1 and θ2 are independent with Poisson(λ1) and Poisson(λ2) distributions, then θ1 + θ2 ∼ Poisson(λ1 + λ2). Simulation for the Poisson distribution (and most discrete distributions) can be cumbersome. Table lookup can be used to invert the cumulative distribution function. Simulation texts describe other approaches.BinomialThe binomial distribution is commonly used to represent the number of ‘successes’ in a sequence of n independent and identically distributed Bernoulli trials, with probability of success p in each trial. A binomial random variable with large n is approximately normal. If θ1 and θ2 are independent with Bin(n1, p) and Bin(n2, p) distributions, then θ1 + θ2 ∼ Bin(n1 + n2, p). For small n, a binomial random variable can be simulated by obtaining n
584 STANDARD PROBABILITY DISTRIBUTIONSindependent standard uniforms and setting θ equal to the number of uniform deviates less than or equal to p. For larger n, more efficient algorithms are often available in computer packages. When n = 1, the binomial is called the Bernoulli distribution.MultinomialThe multinomial distribution is a multivariate generalization of the binomial distribution.The marginal distribution of a single θi is binomial. The conditional distribution of a sub-vector of θ is multinomial with ‘sample size’ parameter reduced by the fixed componentsof θ and ‘probability’ parameters rescaled to have sum equal to one. We can simulate amultivariate draw using a sequence of binomial draws. Draw θ1 from a Bin(n,p1) distri-bution. Then draw θ2,...,θk−1 in order, as follows. For j = 2,...,k − 1, draw θj from aBin(n−􏰌j−1θi, pj/􏰌k pi)distribution. Finally,setθk =n−􏰌k−1θi. Ifatanytime i=1 i=j i=1in the simulation the binomial sample size parameter equals zero, use the convention that a Bin(0, p) variable is identically zero.Negative binomialThe negative binomial distribution is the marginal distribution for a Poisson random vari-able when the rate parameter has a Gamma(α, β) prior distribution. The negative binomialcan also be used as a robust alternative to the Poisson distribution, because it has the samesample space, but has an additional parameter. To simulate a negative binomial randomvariable, draw λ ∼ Gamma(α,β) and then draw θ ∼ Poisson(λ). In the limit α → ∞,and α/β → constant, the distribution approaches a Poisson with parameter α/β. Underthe alternative parameterization, p = β , the random variable θ can be interpreted as β+1the number of Bernoulli failures obtained before the α successes, where the probability of success is p.Beta-binomialThe beta-binomial arises as the marginal distribution of a binomial random variable when the probability of success has a Beta(α,β) prior distribution. It can also be used as a robust alternative to the binomial distribution. The mixture definition gives an algorithm for simulating from the beta-binomial: draw φ ∼ Beta(α, β) and then draw θ ∼ Bin(n, φ).A.3 Bibliographic noteMany software packages contain subroutines to simulate draws from these distributions. Texts on simulation typically include information about many of these distributions; for example, Gentle (2003) discusses simulation of all of these in detail, except for the LKJ distribution. Ripley (1987) is another helpful general book on simulation. Johnson and Kotz (1972) give more detail, such as the characteristic functions, for the distributions. Fortran and C programs for uniform, normal, gamma, Poisson, and binomial distributions are available in Press et al. (1986).
