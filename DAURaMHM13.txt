CHAPTER 13Multilevel linear models: varying slopes, non-nested models, and other complexitiesThis chapter considers some generalizations of the basic multilevel regression. Mod- els in which slopes and intercepts can vary by group (for example, yi = αj[i] + βj[i]xi +···, where α and β both vary by group j; see Figure 11.1c on page 238) can also be interpreted as interactions of the group index with individual-level pre- dictors.Another direction is non-nested models, in which a given dataset can be struc- tured into groups in more than one way. For example, persons in a national survey can be divided by demographics or by states. Responses in a psychological experi- ment might be classified by person (experimental subject), experimental condition, and time.The chapter concludes with some examples of models with nonexchangeable mul- tivariate structures. We continue with generalized linear models in Chapters 14–15 and discuss how to fit all these models in Chapters 16–19.13.1 Varying intercepts and slopesThe next step in multilevel modeling is to allow more than one regression coefficient to vary by group. We shall illustrate with the radon model from the previous chap- ter, which is relatively simple because it only has a single individual-level predictor, x (the indicator for whether the measurement was taken on the first floor).We begin with a varying-intercept, varying-slope model including x but without the county-level uranium predictor; thus,yi ∼ N(αj[i]+βj[i]xi,σy2),fori=1,...,nα j ∼ N μ α , σ α2 ρ σ α σ β , f o r j = 1 , . . . , J , ( 1 3 . 1 )β j μ β ρ σ α σ β σ β2with variation in the αj ’s and the βj ’s and also a between-group correlation param-eter ρ. In R:M3 <- lmer (y ~ x + (1 + x | county))display (M3)which yieldslmer(formula = y ~ x + (1 + x | county)) coef.est coef.seR codeR output(Intercept) 1.46 x -0.68 Error terms:0.05 0.09Std.Dev. Corr x 0.34 -0.34Groups Namecounty (Intercept) 0.35279
R code R outputR code R outputR code R outputResidual 0.75# of obs: 919, groups: county, 85 deviance = 2161.1In this model, the unexplained within-county variation has an estimated standard deviation of σˆy = 0.75; the estimated standard deviation of the county intercepts is σˆα = 0.35; the estimated standard deviation of the county slopes is σˆβ = 0.34; and the estimated correlation between intercepts and slopes is −0.34.We then can typecoef (M3)to yield$county(Intercept) x280 VARYING SLOPES AND NON-NESTED MODELS1 1.142 0.933 1.47...85 1.38-0.54 -0.77 -0.67-0.65Or we can separately look at the estimated population mean coefficients μα , μβ and then the estimated errors for each county. First, we typefixef (M3)to see the estimated average coefficients (“fixed effects”):(Intercept) x 1.46 -0.68Then, we typeranef (M3)to see the estimated group-level errors (“random effects”):-0.08 0.03We can regain the estimated intercept and slope αj , βj for each county by simply adding the errors to μα and μβ; thus, the estimated regression line for county 1 is (1.46 − 0.32) + (−0.68 + 0.14)x = 1.14 − 0.54x, and so forth.The group-level model for the parameters (αj,βj) allows for partial pooling in the estimated intercepts and slopes. Figure 13.1 shows the results—the estimated lines y = αj + βj x—for the radon data in eight different counties.Including group-level predictorsWe can expand the model of (α, β) in (13.1) by including a group-level predictor (in this case, soil uranium):α j ∼ N γ 0α + γ 1α u j , σ α2 ρ σ α σ β , f o r j = 1 , . . . , J . ( 1 3 . 2 ) β j γ 0β + γ 1β u j ρ σ α σ β σ β2The resulting estimates for the αj’s and βj’s are changed slightly from what is displayed in Figure 13.1, but more interesting are the second-level models them- selves, whose estimates are shown in Figure 13.2. Here is the result of fitting the model in R:123 ... 85(Intercept) x -0.32 0.14 -0.53 -0.09 0.01 0.01
VARYING INTERCEPTS AND SLOPES 281LAC QUI PARLE AITKIN KOOCHICHING DOUGLAS􏱺􏱺􏱺􏱺􏶸􏴣 􏶸􏴣 􏶸􏴣 􏶸􏴣CLAY STEARNS RAMSEY ST LOUIS􏱺􏱺􏱺􏱺􏶸􏴣 􏶸􏴣 􏶸􏴣 􏶸􏴣Figure 13.1 Multilevel (partial pooling) regression lines y = αj + βj x, displayed for eight counties j. In this model, both the intercept and the slope vary by county. The light solid and dashed lines show the no-pooling and complete pooling regression lines. Compare to Figure 12.4, in which only the intercept varies.−􏲄 −􏲄 −􏲄 −􏲄􏴘 􏲄􏴜 −􏲄 −􏲄 −􏲄 −􏲄􏴘 􏲄􏴜 􏲦􏱨􏱉−􏵔􏵕􏲇􏲳􏱴􏶾􏱚􏱳􏶿􏲳 􏲦􏱨􏱉−􏵔􏵕􏲇􏲳􏱴􏶾􏱚􏱳􏶿􏲳Figure 13.2 (a) Estimates ± standard errors for the county intercepts αj, plotted versus county-level uranium measurement uj, along with the estimated multilevel regression line, α = γ0α + γ1αu. (b) Estimates ± standard errors for the county slopes βj, plotted versus county-level uranium measurement uj, along with the estimated multilevel regression line, β = γ0β + γ1β u. Estimates and standard errors are the posterior medians and standard deviations, respectively. For each graph, the county coefficients roughly follow the line but not exactly; the discrepancies of the coefficients from the line are summarized by the county-level standard-deviation parameters σα,σβ.lmer(formula = y ~ x + u.full + x:u.full + (1 + x | county)) coef.est coef.se(Intercept) 1.47 0.04R output􏲧􏴇􏱒􏳺􏲗􏱦􏱬􏲙􏴴􏶺 􏲄􏴘􏳉􏴘􏱗􏲽􏲓􏶹􏱧􏵔􏵕 􏱗􏲽􏲓􏶹􏱧􏵔􏵕 −􏴙 −􏴙􏲧􏴇􏱒􏳺􏲗􏰼􏱗􏷀 −􏳉 −􏳉 − 􏲄 􏴘􏱗􏲽􏲓􏶹􏱧􏵔􏵕 􏱗􏲽􏲓􏶹􏱧􏵔􏵕 −􏴙 −􏴙􏱗􏲽􏲓􏶹􏱧􏵔􏵕 􏱗􏲽􏲓􏶹􏱧􏵔􏵕 −􏴙 −􏴙􏱗􏲽􏲓􏶹􏱧􏵔􏵕 􏱗􏲽􏲓􏶹􏱧􏵔􏵕 −􏴙 −􏴙xu.full x:u.full Error terms:Groups Namecounty (Intercept) 0.12-0.67 0.08 0.81 0.09 -0.42 0.23Std.Dev. Corrx 0.31 0.41 0.75Residual# of obs: 919, groups: county, 85 deviance = 2114.3The parameters γ0α, γ0β, γ1α, γ1β in model (13.2) are the coefficients for the intercept,
R outputR outputR output282 VARYING SLOPES AND NON-NESTED MODELSx, u.full, and x:u.full, respectively, in the regression. In particular, the inter- action corresponds to allowing uranium to be a predictor in the regression for the slopes.The estimated coefficients in each group (from coef(M4)) are:$county(Intercept) x u.full x:u.fullR codeThe coefficients for the intercept and x vary, as specified in the model. This can be compared to the model on page 267 in which only the intercept varies.Going from lmer output to intercepts and slopesAs before, we can combine the average coefficients with the group-level errors to compute the intercepts αj and slopes βj of model (13.2). For example, the fitted regression model in county 85 is yi = 1.47 − 0.67xi + 0.81u85 − 0.42xiu85 − 0.02 − 0.03xi. The log uranium level in county 85, u85, is 0.36, and so the fitted regression line in county 85 is yi = 1.73 − 0.85xi. More generally, we can compute a vector of county intercepts α and slopes β:a.hat.M4 <- coef(M4)[,1] + coef(M4)[,3]*u b.hat.M4 <- coef(M4)[,2] + coef(M4)[,4]*uHere it is actually useful to have the variable u defined at the county level (as compared to u.full = u[county] which was used in the lmer() call). We next consider these linear transformations algebraically.Varying slopes as interactionsSection 12.5 gave multiple ways of writing the basic multilevel model. These same ideas apply to models with varying slopes, which can be considered as interactions between group indicators and an individual-level predictor. For example, consider the model with an individual-level predictor xi and a group-level predictor uj,yi = αj[i] + βj[i]xi + i αj = γ0α+γ1αuj+ηjαβj = γ0β+γ1βuj+ηjβ.We can re-express this as a single model by substituting the formulas for αj andβj into the equation for yi:yi = γα +γαu +ηα + γβ +γβu +ηβ xi +i. (13.3) 0 1 j[i] j[i] 0 1 j[i] j[i]1 1.46 -0.65 0.812 1.50 -0.89 0.81...85 1.44 -0.70 0.81-0.42 -0.42-0.42Or we can display the average coefficients (using fixef(M4)): (Intercept) x u.full x:u.full1.47 -0.67 0.81 -0.42and the group-level errors for the intercepts and slopes (using ranef(M4)): (Intercept) x12 ... 85-0.01 0.02 0.03 -0.21-0.02 -0.03
VARYING SLOPES WITHOUT VARYING INTERCEPTS 283This expression looks messy but it is really just a regression including various interactions. If we define a new individual-level predictor vi = uj[i] (in the radon example, this is the uranium level in the county where your house is located), we can re-express (13.3) term by term asyi =a+bvi +cj[i] +dxi +evixi +fj[i]xi +i. This can be thought of in several ways:• A varying-intercept, varying-slope model with four individual-level predictors (the constant term, vi, xi, and the interaction vixi) and varying intercepts and slopes that are centered at zero.• A regression model with 4 + 2J predictors: the constant term, vi, xi, vixi, indi- cators for the J groups, and interactions between x and the J group indicators.• A regression model with four predictors and three error terms.• Or, to go back to the original formulation, a varying-intercept, varying-slopemodel with one group-level predictor.Which of these expressions is most useful depends on the context. In the radon analysis, where the goal is to predict radon levels in individual counties, the varying- intercept, varying-slope formulation, as pictured in Figure 13.2, seems most appro- priate. But in a problem where interest lies in the regression coefficients for xi, uj, and their interaction, it can be more helpful to focus on these predictors and consider the unexplained variation in intercepts and slopes merely as error terms.13.2 Varying slopes without varying interceptsFigure 11.1 on page 238 displays a varying-intercept model, a varying-slope model, and a varying-intercept, varying-slope model. Almost always, when a slope is al- lowed to vary, it makes sense for the intercept to vary also. That is, the graph in the center of Figure 11.1b usually does not make sense. For example, if the coeffi- cient of floor varies with county, then it makes sense to allow the intercept of the regression to vary also. It would be an implausible scenario in which the counties were all identical in radon levels for houses without basements, but differed in their coefficients for x.A situation in which a constant-intercept, varying-slope model is appropriateOccasionally it is reasonable to allow the slope but not the intercept to vary by group. For example, consider a study in which J separate experiments are performed on samples from a common population, with each experiment randomly assigning a control condition to half its subjects and a treatment to the other half. Further suppose that the “control” conditions are the same for each experiment but the “treatments” vary. In that case, it would make sense to fix the intercept and allow the slope to vary—thus, a basic model of:yi ∼ N(α+θj[i]Ti,σy2)θ j ∼ N ( μ θ , σ θ2 ) , ( 1 3 . 4 )where Ti = 1 for treated units and 0 for controls. Individual-level predictors could be added to the regression for y, and any interactions with treatment could also
R codeR codeacterizing the treatments.Fitting in RTo fit such a model in lmer(), we must explicitly remove the intercept from the group of coefficients that vary by group; for example, here is model (13.4) including the treatment indicator T as a predictor:lmer (y ~ T + (T - 1 | group))The varying slope allows a different treatment effect for each group. And here is model (13.5) with an individual-level predictor x:lmer (y ~ x + T + (T + x:T - 1 | group))Here, the treatment effect and its interaction with x vary by group. 13.3 Modeling multiple varying coefficients using the scaledinverse-Wishart distributionWhen more than two coefficients vary (for example, yi ∼ N(β0 +β1Xi1 +β2Xi2, σ2), with β0, β1, and β2 varying by group), it is helpful to move to matrix notation in modeling the coefficients and their group-level regression model and covariance matrix.Simple model with two varying coefficients and no group-level predictorsStarting with the model that begins this chapter, we can rewrite the basic varying- intercept, varying-slope model (13.1) in matrix notation asyi ∼ N(XiBj[i], σy2), for i = 1,...,nBj ∼ N(MB,ΣB), for j = 1,...,J, (13.6)where• X is the n × 2 matrix of predictors: the first column of X is a column of 1’s (that is, the constant term in the regression), and the second column is the predictor x. Xi is then the vector of length 2 representing the ith row of X, and XiBj[i] is simply αj[i] + βj[i]xi from the top line of (13.1).• B = (α, β) is the J × 2 matrix of individual-level regression coefficients. For any group j, Bj is a vector of length 2 corresponding to the jth row of B (although for convenience we consider Bj as a column vector in the product XiBj[i] in model (13.6)). The two elements of Bj correspond to the intercept and slope, respectively, for the regression model in group j. Bj[i] in the first line of (13.6) is the j[i]th row of B, that is, the vector representing the intercept and slope for the group that includes unit i.• MB = (μα , μβ ) is a vector of length 2, representing the mean of the distribution of the intercepts and the mean of the distribution of the slopes.284 VARYING SLOPES AND NON-NESTED MODELS have varying slopes; for example,θ 1 , j θ2,jyi∼∼N (α + βxi + θ1,j[i]Ti + β2,j[i]xiTi, σy2)N μ 1 , σ 12 ρ σ 1 σ 2 , f o r j = 1 , . . . , J , ( 1 3 . 5 )μ2 ρσ1σ2 σ2The multilevel model could be further extended with group-level predictors char-
INVERSE-WISHART MODEL FOR VARYING INTERCEPTS AND SLOPES 285• ΣB is the 2 × 2 covariance matrix representing the variation of the intercepts and slopes in the population of groups, as in the second line of (13.1).We are following our general notation in which uppercase letters represent matrices: thus, the vectors α and β are combined into the matrix B.In the fitted radon model on page 279, the parameters of the group-level model are estimated at MB = (1.46,−0.68) and ΣB = σˆa2 ρˆσˆaσˆb , where σˆa = 0.35,ρˆ σˆ a σˆ b σˆ b2σˆb = 0.34, and ρˆ = −0.34. The estimated coefficient matrix B is given by the 85 × 2array at the end of the display of coef(M3) on page 280. More than two varying coefficientsThe same expression as above holds, except that the 2’s are replaced by K’s, where K is the number of individual-level predictors (including the intercept) that vary by group. As we discuss shortly in the context of the inverse-Wishart model, estimation becomes more difficult when K > 2 because of constraints among the correlation parameters of the covariance matrix ΣB.Including group-level predictorsMore generally, we can have J groups, K individual-level predictors, and L pre- dictors in the group-level regression (including the constant term as a predictor in both cases). For example, K = L = 2 in the radon model that has floor as an individual predictor and uranium as a county-level predictor.We can extend model (13.6) to include group-level predictors: yi ∼ N(XiBj[i],σy2), fori=1,...,nBj ∼ N(UjG,ΣB), for j = 1,...,J, (13.7)where B is the J × K matrix of individual-level coefficients, U is the J × L matrix of group-level predictors (including the constant term), and G is the L × K matrix of coefficients for the group-level regression. Uj is the jth row of U, the vector of predictors for group j, and so UjG is a vector of length K.Model (13.1) is a special case with K = L = 2, and the coefficients in G are then γ0α, γ0β, γ1α, γ1β. For the fitted radon model on page 279, the γ’s are the four unmodeled coefficients (for the intercept, x, u.full, and x:u.full, respectively), and the two columns of the estimated coefficient matrix B are estimated by a.hat and b.hat, as defined by the R code on page 282.Including individual-level predictors whose coefficients do not vary by groupThe model can be further expanded by adding unmodeled individual-level coeffi- cients, so that the top line of (13.7) becomesy i ∼ N ( X i0 β 0 + X i B j [ i ] , σ y2 ) , f o r i = 1 , . . . , n , ( 1 3 . 8 )where X0 is a matrix of these additional predictors and β0 is the vector of their regression coefficients (which, by assumption, are common to all the groups).Model (13.8) is sometimes called a mixed-effects regression, where the β0’s and the B’s are the fixed and random effects, respectively. As noted on pages 2 and 245, we avoid these terms because of their ambiguity in the statistical literature. For example, sometimes unvarying coefficients such as the β0’s in model (13.8) are called “fixed,” but sometimes the term “fixed effects” refers to intercepts that vary
286 VARYING SLOPES AND NON-NESTED MODELSby groups but are not given a multilevel model (this is what we call the “no-pooling model,” as pictured, for example, by the solid lines in Figure 12.2 on page 255).Equivalently, model (13.8) can be written by folding X0 and X into a common predictor matrix X, folding β0 and B into a common coefficient matrix B, and using model (13.1), with the appropriate elements in ΣB set to zero, implying no variation among groups for certain coefficients.Modeling the group-level covariance matrix using the scaled inverse-Wishart distributionWhen the number K of varying coefficients per group is more than two, modeling the correlation parameters ρ is a challenge. In addition to each of the correlations being restricted to fall between −1 and 1, the correlations are jointly constrained in a complicated way—technically, the covariance matrix Σβ must be positive definite. (An example of the constraint is: if ρ12 = 0.9 and ρ13 = 0.9, then ρ23 must be at least 0.62.)Modeling and estimation are more complicated in this jointly constrained space. We first introduce the inverse-Wishart model, then generalize to the scaled inverse- Wishart, which is what we recommend for modeling the covariance matrix of the distribution of varying coefficients.Inverse-Wishart model. One model that has been proposed for the covariance matrix Σβ is the inverse-Wishart distribution, which has the advantage of being computationally convenient (especially when using Bugs, as we illustrate in Section 17.1) but the disadvantage of being difficult to interpret.In the model ΣB ∼ Inv-WishartK+1(I), the two parameters of the inverse- Wishart distribution are the degrees of freedom (here set to K+1, where K is the dimension of B, that is, the number of coefficients in the model that vary by group) and the scale (here set to the K × K identity matrix).To understand this model, we consider its implications for the standard deviation and correlations. Recall that if there are K varying coefficients, then ΣB is a K × K matrix, with diagonal elements Σkk = σk2 and off-diagonal-elements Σkl = ρklσkσl (generalizing models (13.1) and (13.2) to K > 2).Setting the degrees-of-freedom parameter to K+1 has the effect of setting a uniform distribution on the individual correlation parameters (that is, they are assumed equally likely to take on any value between −1 and 1).Scaled inverse-Wishart model. When the degrees of freedom parameter of the inverse-Wishart distribution is set to K+1, the resulting model is reasonable for the correlations but is quite constraining on the scale parameters σk. This is a prob- lem because we would like to estimate σk from the data. Changing the degrees of freedom allows the σk’s to be estimated more freely, but at the cost of constraining the correlation parameters.We get around this problem by expanding the inverse-Wishart model with a new vector of scale parameters ξk:ΣB = Diag(ξ)QDiag(ξ),with the unscaled covariance matrix Q being given the inverse-Wishart model:Q ∼ Inv-WishartK+1(I).The variances then correspond to the diagonal elements of the unscaled covariance
UNDERSTANDING GROUP-LEVEL CORRELATIONS 287blacks hispanics whites others􏰮􏱋􏰯􏱌 􏰮􏱋􏰯􏱌 􏰮􏱋􏰯􏱌 􏰮􏱋􏰯􏱌 􏱃􏱍􏱎􏱏􏱐􏱑􏱒 􏱃􏱍􏱎􏱏􏱐􏱑􏱒 􏱃􏱍􏱎􏱏􏱐􏱑􏱒 􏱃􏱍􏱎􏱏􏱐􏱑􏱒Figure 13.3 Multilevel regression lines y = αj +βj x for log earnings on height (among those with positive earnings), in four ethnic categories j. The gray lines indicate uncertainty in the fitted regressions.􏶲􏳜􏱐􏲍􏰸􏷂􏷃αFigure 13.4 Scatterplot of estimated intercepts and slopes (for whites, hispanics, blacks, and others), (αj,βj), for the earnings-height regressions shown in Figure 13.3. The ex- treme negative correlation arises because the center of the range of height is far from zero. Compare to the coefficients in the rescaled model, as displayed in Figure 13.7.matrix Q, multiplied by the appropriate scaling factors ξ: σk2 = Σkk = ξk2Qkk, for k = 1,...,K,and the covariances areΣkl = ξkξlQkl, for k,l = 1,...,K,We prefer to express in terms of the standard deviations,􏴩􏱯􏷄β􏲄􏰳 􏲄􏱼 􏲄􏱿 􏲄􏲀􏱗􏲽􏱅􏴁􏱐􏴂 􏷁􏱗􏲽􏱅􏴁􏱐􏴂 􏷁􏱗􏲽􏱅􏴁􏱐􏴂 􏷁􏱗􏲽􏱅􏴁􏱐􏴂 􏷁and correlations­σk=|ξk| Qkk, ρkl = Σkl/(σkσl).The parameters in ξ and Q cannot be interpreted separately: they are a convenient way to set up the model, but it is the standard deviations σk and the correlations ρkl that are of interest (and which are relevant for producing partially pooled estimates for the coefficients in B).As with the unscaled Wishart, the model implies a uniform distribution on the correlation parameters. As we discuss next, it can make sense to transform the data to remove any large correlations that could be expected simply from the structure of the data.13.4 Understanding correlations between group-level intercepts and slopesRecall that varying slopes can be interpreted as interactions between an individual- level predictor and group indicators. As with classical regression models with in- teractions, the intercepts can often be more clearly interpreted if the continuous
288 VARYING SLOPES AND NON-NESTED MODELS􏲉􏱃􏳅 􏴠􏶔􏷅􏵍􏶟􏱴􏳼 􏶡􏲺􏱒􏱿􏲀􏳗 􏱃􏱍􏱎􏱏􏱐􏱑􏱒Figure 13.5 Sketch illustrating the difficulty of simultaneously estimating α and β. The lines show the regressions for the four ethnic groups as displayed in Figure 13.3: the center of the range of x values is far from zero, and so small changes in the slope induce large changes in the intercept.blacks hispanics whites others−􏰴 −􏰴 −􏰴 −􏰴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴Figure 13.6 Multilevel regression lines y = αj + βj z, for log earnings given mean-adjusted height (zi = xi − x ̄), in four ethnic groups j. The gray lines indicate uncertainty in the fitted regressions.predictor is appropriately centered. We illustrate with the height and earnings ex- ample from Chapter 4.We begin by fitting a multilevel model of log earnings given height, allowing the coefficients to vary by ethnicity. The data and fitted model are displayed in Figure 13.3. (Little is gained by fitting a multilevel model here—with only four groups, a classical no-pooling model would work nearly as well, as discussed in Section 12.9—but this is a convenient example to illustrate a general point.)Figure 13.4 displays the estimates of (αj , βj ) for the four ethnic groups, and they have a strong negative correlation: the groups with high values of α have relatively low values of β, and vice versa. This correlation occurs because the center of the x-values of the data is far from zero. The regression lines have to go roughly through the center of the data, and then changes in the slope induce opposite changes in the intercept, as illustrated in Figure 13.5.There is nothing wrong with a high correlation between the α’s and β’s, but it makes the estimated intercepts more difficult to interpret. As with interaction models in classical regression, it can be helpful to subtract the average value of the continuous x before including it in the regression; thus, yi ∼ N(αj[i] + βj[i]zi, σy2), where zi = xi − x ̄. Figures 13.6 and 13.7 show the results for the earnings regression: the correlation has pretty much disappeared. Centering the predictor x will not necessarily remove correlations between intercepts and slopes—but any correlation that remains can then be more easily interpreted. In addition, centering can speed convergence of the Gibbs sampling algorithm used by Bugs and other software.We fit this model, and the subsequent models in this chapter, in Bugs (see Chap-􏱗􏲽􏱅􏴁􏱐􏴂 􏷁􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏷁􏱢􏲫􏱗􏲽􏱅􏴁􏱐􏴂 􏷁􏱗􏲽􏱅􏴁􏱐􏴂 􏷁
NON-NESTED MODELS289􏷆􏰮 􏷆􏳎 􏷆􏷁 􏷆􏳛 􏱐􏲍􏰸􏷂􏷃αFigure 13.7 Scatterplot of estimated intercepts and slopes, (αj,βj), for the regression of earnings on mean-adjusted height z, for the four groups j displayed in Figure 13.6. The coefficients are no longer strongly correlated (compare to Figure 13.4).ter 17 for examples of code) because, as discussed in Section 12.4, the current version of lmer() does not work so well when the number of groups is small—and, conversely, with these small datasets, Bugs is not too slow.13.5 Non-nested modelsSo far we have considered the simplest hierarchical structure of individuals i in groups j. We now discuss models for more complicated grouping structures such as introduced in Section 11.3.Example: a psychological experiment with two potentially interacting factorsFigure 13.8 displays data from a psychological experiment of pilots on flight simu- lators, with n = 40 data points corresponding to J = 5 treatment conditions and K = 8 different airports. The responses can be fit to a non-nested multilevel model of the formyi ∼ N(μ+γj[i]+δk[i],σy2),fori=1,...,nγj ∼ N(0,σγ2), forj=1,...,Jδk ∼ N(0,σδ2), for k = 1,...,K. (13.9)The parameters γj and δk represent treatment effects and airport effects. Their distributions are centered at zero (rather than given mean levels μγ,μδ) because the regression model for y already has an intercept, μ, and any nonzero mean for the γ and δ distributions could be folded into μ. As we shall see in Section 19.4, it can sometimes be effective for computational purposes to add extra mean-level parameters into the model, but the coefficients in this expanded model must be interpreted with care.We can perform a quick fit as follows:lmer (y ~ 1 + (1 | group.id) + (1 | scenario.id))where group.id and scenario.id are the index variables for the five treatment conditions and eight airports, respectively.When fit to the data in Figure 13.8, the estimated residual standard deviations at the individual, treatment, and airport levels are σˆy = 0.23, σˆγ = 0.04, and σˆδ = 0.32. Thus, the variation among airports is huge—even larger than that among individual measurements—but the treatments vary almost not at all. This general pattern can be seen in Figure 13.8.R code􏱯􏷄β􏲄􏰳 􏲄􏱼 􏲄􏱿 􏲄􏲀
290 VARYING SLOPES AND NON-NESTED MODELS􏷋􏶁􏷌 􏷍􏵶 􏲺􏷎􏷏􏵓􏶂 􏷐􏲑􏷑 􏵴􏴾􏷒􏷓􏵔􏶹 􏲣􏰶􏱗􏷔Figure 13.8 Success rates of pilots training on a flight simulator with five different treat- ments and eight different airports. Shadings in the 40 cells i represent different success rates yi, with black and white corresponding to 0 and 100%, respectively. For convenience in reading the display, the treatments and airports have each been sorted in increasing order of average success. These 40 data points have two groupings—treatments and airports— which are not nested.airportData in matrix form treatment conditionsData in vector formyjk0.38 1 1 0.00 1 2 0.38 1 3 0.00 1 4 0.33 1 5 1.00 1 6 0.12 1 7 1.00 1 8 0.25 2 11 0.382 0.003 0.384 0.005 0.336 1.007 0.128 1.000.25 0.50 0.14 0.43 0.00 0.67 0.00 0.00 0.50 0.33 0.71 0.29 0.12 0.00 0.00 0.86 0.50 0.14 0.29 0.86 1.00 1.00 1.00 0.86 0.12 0.00 0.14 0.14 0.86 1.00 1.00 0.75... ... ...Figure 13.9 Data from Figure 13.8 displayed as an array (yjk) and in our preferred nota-tion as a vector (yi) with group indicators j[i] and k[i].Model (13.9) can also be written more cleanly as yjk ∼ N(μ+γj +δk,σy2), but we actually prefer the more awkward notation using j[i] and k[i] because it emphasizes the multilevel structure of the model and is not restricted to balanced designs. When modeling a data array of the form (yjk), we usually convert it into a vector with index variables for the rows and columns, as illustrated in Figure 13.9 for the flight simulator data.Example: regression of earnings on ethnicity categories, age categories, and heightAll the ideas of the earlier part of this chapter, introduced in the context of a simple structure of individuals within groups, apply to non-nested models as well. For example, Figure 13.10 displays the estimated regression of log earnings, yi, on height, zi (mean-adjusted, for reasons discussed in the context of Figures 13.3– 13.6), applied to the J = 4 ethnic groups and K = 3 age categories. In essence, there is a separate regression model for each age group and ethnicity combination. The multilevel model can be written, somewhat awkwardly, as a data-level model,y i ∼ N ( α j [ i ] , k [ i ] + β j [ i ] , k [ i ] z i , σ y2 ) , f o r i = 1 , . . . , n ,􏷇 􏷈 􏷉 􏷊 􏱐􏶸􏱍􏱎
NON-NESTED MODELS 291blacks, age 18−34 hispanics, age 18−34 whites, age 18−34 others, age 18−34−􏰴 −􏰴 −􏰴 −􏰴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴blacks, age 35−49 hispanics, age 35−49 whites, age 35−49 others, age 35−49−􏰴 −􏰴 −􏰴 −􏰴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴blacks, age 50−64 hispanics, age 50−64 whites, age 50−64 others, age 50−64−􏰴 −􏰴 −􏰴 −􏰴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴 􏱃􏱍􏱎􏱏􏱐􏱑􏱒􏱀􏵦􏱚􏱳􏱴Figure 13.10 Multilevel regression lines y = βj0,k + βj1,kz, for log earnings y given mean- adjusted height z, for four ethnic groups j and three age categories k. The gray lines indicate uncertainty in the fitted regressions.a decomposition of the intercepts and slopes into terms for ethnicity, age, and ethnicity × age,μ0 μ1γeth0j ∼N􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱢􏲫􏱢􏲫􏱢􏲫􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱢􏲫􏱢􏲫􏱢􏲫􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱢􏲫􏱢􏲫􏱢􏲫􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱗􏲽􏱅􏴁􏱐􏴂 􏱢􏲫􏱢􏲫􏱢􏲫αj,k βj,kγ eth γ age γ eth×age 0j 0k 0jk= and models for variation,+ eth + age + eth×age , γ1j γ1k γ1jkγeth 1j0 00 0,Σeth , forj=1,...,Jγ age 0k00 ,Σageγ age ∼N 1kγ eth×age0jk ∼Nγ eth×age 1jk, fork=1,...,K, Σeth×age , for j = 1,...,J; k = 1,...,K.Because we have included means μ0, μ1 in the decomposition above, we can center each batch of coefficients at 0.Interpretation of data-level variance. The data-level errors have estimated resid- ual standard deviation σˆy = 0.87. That is, given ethnicity, age group, and height, log earnings can be predicted to within approximately ±0.87, and so earnings them- selves can be predicted to within a multiplicative factor of e0.87 = 2.4. So earnings cannot be predicted well at all by these factors, which is also apparent from the scatter in Figure 13.10.Interpretation of group-level variances. The group-level errors can be separated into intercept and slope coefficients. The intercepts have estimated residual stan-
292VARYING SLOPES AND NON-NESTED MODELSB: 257 D: 245 E: 182 A: 203 C: 231Figure 13.11 Data from a 5 × 5treatments on the yields of millet crops, from Snedecor and Cochran (1989). Each cell shows the randomly assigned treatment and the observed yield for the plot.dard deviations of (Σeth)1/2 = 0.08 at the ethnicity level, (Σage)1/2 = 0.25 at the 00 00age level, and (Σeth×age)1/2 = 0.11 at the ethnicity × age level. Because we have 00rescaled height to have a mean of zero (see Figure 13.10), we can interpret these standard deviations as the relative importance of each factor (ethnicity, age group, and their interaction) on log earnings at the average height in the population.This model fits earnings on the log scale and so these standard deviations can be interpreted accordingly. For example, the residual standard deviation of 0.08 for the ethnicity coefficients implies that the predictive effects of ethnic groups in the model are on the order of ±0.08, which correspond to multiplicative factors from about e−0.08 = 0.92 to e0.08 = 1.08.The slopes have estimated residual standard deviations of (Σeth)1/2 = 0.03 at 11the ethnicity level, (Σage)1/2 = 0.02 at the age level, and (Σeth×age)1/2 = 0.02 at 11 11the ethnicity × age level. These slopes are per inch of height, so, for example, the predictive effects of ethnic groups in the model are in the range of ±3% in income per inch of height. One can also look at the estimated correlation between intercepts and slopes for each factor.Example: a latin square design with grouping factors and group-level predictorsNon-nested models can also include group-level predictors. We illustrate with data from a 5×5 latin square experiment, a design in which 25 units arranged in a square grid are assigned five different treatments, with each treatment being assigned to one unit in each row and each column. Figure 13.11 shows the treatment assignments and data from a small agricultural experiment. There are three non-nested levels of grouping—rows, columns, and treatments—and each has a natural group-level predictor corresponding to a linear trend. (The five treatments are ordered.)E: 230 A: 283 B: 252 C: 204 D: 271A: 279 E: 245 C: 280 D: 227 B: 266C: 287 B: 280 D: 246 E: 193 A: 334D: 202 C: 260 A: 250 B: 259 E: 338latin square experiment studying the effects of five orderedThe corresponding multilevel model can be written asyi ∼ N(μ+βrow+βcolumn+βtreat,σ2),fori=1,...,25βrow ∼ jβcolumn∼ kβtreat∼ N(γtreat·(l−3),σ2l β treatj[i] k[i] N(γrow · (j − 3), σ2l[i] y), for j = 1, . . . , 5N(γcolumn ·(k−3),σ2β columnβ rowThus j, k, and l serve simultaneously as values of the row, column, and treatment predictors.By subtracting 3, we have centered the row, column, and treatment predictors at zero; the parameter μ has a clear interpretation as the grand mean of the data, with the different β’s supplying deviations for rows, columns, and treatments. As with group-level models in general, the linear trends at each level potentially allow more precise estimates of the group effects, to the extent that these trends are supported by the data. An advantage of multilevel modeling here is that it doesn’t force a), for k =1,...,5 ), forl=1,...,5.(13.10)
SELECTING, TRANSFORMING, AND COMBINING INPUTS 293row effects column effects treatment effects􏲫􏴢􏲫􏴢􏲫􏴢􏵦 􏲦􏵘􏷕 􏴾􏱅􏶌􏰹Figure 13.12 Estimates ±1 standard error for the row, column, and treatment effects for the latin square data in Figure 13.11. The five levels of each factor are ordered, and the lines display the estimated group-level regressions, y=μ+γrow·(x−3), y =μ+γcolumn·(x−3), and y=μ+γtreat·(x−3).choice between a linear fit and separate estimates for each level of a predictor. (This is an issue we discussed more generally in Chapter 11 in the context of including group indicators as well as group-level predictors.)Figure 13.12 shows the estimated row, column, and treatment effects on graphs, along with the estimated linear trends. The grand mean μ has been added back to each of these observations so that the plots are on the scale of the original data. This sort of data structure is commonly studied using the analysis of variance, whose connections with multilevel models we discuss fully in Chapter 22, including a discussion of this latin square example in Section 22.5.13.6 Selecting, transforming, and combining regression inputsAs with classical regression (see Section 4.5), choices must be made in multilevel models about which input variables to include, and how best to transform and combine them. We discuss here how some of these decisions can be expressed as particular choices of parameters in a multilevel model. The topic of formalizing modeling choices is currently an active area of research—key concerns include using information in potential input variables without being overwhelmed by the com- plexity of the relating model, and including model choice in uncertainty estimates. As discussed in Section 9.5, the assumption of ignorability in observational studies is more plausible when controlling for more pre-treatment inputs, which gives us a motivation to include more regression predictors.Classical models for regression coefficientsMultilevel modeling includes classical least squares regression as a special case. In a multilevel model, each coefficient is part of a model with some mean and standard deviation. (These mean values can themselves be determined by group- level predictors in a group-level model.) In classical regression, every predictor is either in or out of the model, and each of these options corresponds to a special case of the multilevel model.• If a predictor is “in,” this corresponds to a coefficient model with standard deviation of ∞: no group-level information is used to estimate this parameter, so it is estimated directly using least squares. It turns out that in this case the group-level mean is irrelevant (see formula (12.16) on page 269 for the case σα = ∞); for convenience we often set it to 0.• If a predictor is “out,” this corresponds to a group-level model with group-level􏰲􏰴􏱸􏰲􏰴􏱸􏰲􏰴􏱸
294 VARYING SLOPES AND NON-NESTED MODELS mean 0 and standard deviation 0: the coefficient estimate is then fixed at zero(see (12.16) for the case σα = 0) with no uncertainty.Multilevel modeling as an alternative to selecting regression predictorsMultilevel models can be used to combine inputs into more effective regression predictors, generalizing some of the transformation ideas discussed in Section 4.6. When many potential regression inputs are available, the fundamental approach is to include as many of these inputs as possible, but not necessarily as independent least squares predictors.For example, Witte et al. (1994) describe a logistic regression in a case-control study of 362 persons, predicting cancer incidence given information on consumption of 87 different foods (and also controlling for five background variables which we do not discuss further here). Each of the foods can potentially increase or decrease the probability of cancer, but it would be hard to trust the result of a regression with 87 predictors fit to only 362 data points, and classical tools for selecting regression predictors do not seem so helpful here. In our general notation, the challenge is to estimate the logistic regression of cancer status y on the 362 × 87 matrix X of food consumption (and the 362 × 6 matrix X0 containing the constant term and the 5 background variables).More information is available, however, because each of the 87 foods can be characterized by its level of each of 35 nutrients, information that can be expressed as an 87 × 36 matrix of predictors Z indicating how much of each nutrient is in each food. Witte et al. fit the following multilevel model:Pr(yi = 1) = logit−1(Xi0β0 + XiBj[i]), for i = 1, . . . , 362Bj ∼ N(Zjγ,σβ2), forj=1,...,87. (13.11)The food-nutrient information in Z allows the multilevel model to estimate separate predictive effects for foods, after controlling for systematic patterns associated with nutrients. In the extreme case that σβ = 0, all the variation associated with the foods is explained by the nutrients. At the other extreme, σβ = ∞ would imply that the nutrient information is not helping at all.Model (13.11) is helpful in reducing the number of food predictors from 87 to 35. At this point, Witte et al. used substantive understanding of diet and cancer to understand the result. Ultimately, we would like to have a model that structures the 35 predictors even more, perhaps by categorizing them into batches or com- bining them in some way. The next example sketches how this might be done; it is currently an active research topic to generally structure large numbers of regression predictors.Linear transformation and combination of inputs in a multilevel modelFor another example, we consider the problem of forecasting presidential elections by state (see Section 1.2). A forecasting model based on 11 recent national elections has more than 500 “data points”—state-level elections—and can then potentially in- clude many state-level predictors measuring factors such as economic performance, incumbency, and popularity. However, at the national level there are really only 11 observations and so one must be parsimonious with national-level predictors. In practice, this means performing some preliminary data analysis to pick a sin- gle economic predictor, a single popularity predictor, and maybe one or two other predictors based on incumbency and political ideology.
SELECTING, TRANSFORMING, AND COMBINING INPUTS 295Setting up a model to allow partial pooling of a set of regression predictorsA more general approach to including national predictors is possible using multilevel modeling. For example, suppose we wish to include five measures of the national economy (for example, change in GDP per capita, change in unemployment, and so forth). The usual approach (which we have followed in the past in this problem) is to choose one of these as the economic predictor, x, thus writing the model asyi = α + βxi + · · · , (13.12)where the dots indicate all the rest of the model, including other state-level and national predictors, as well as error terms at the state, regional, and national levels. Here we focus on the economic inputs, for simplicity setting aside the rest of the model.Instead of choosing just one of the five economic inputs, it would perhaps be better first to standardize each of them (see Section 4.2), orient them so they are in the same direction, label these standardized variables as X(j), for j = 1, . . . , 5, and then average them into a single predictor, defined for each data point asavg 15or, equivalently,measure chosen before.However, model (13.13) is limited in that it restricts the coefficients of the fiveseparate xj’s to be equal. More generally, we can replace (13.13) by a weighted average:(13.14)(13.15)xi = 5yi = α+βxavg+···Xij, for i = 1,....n. (13.13) This new xavg can be included in place of x as the regression predictor in (13.12),j=1i= α+1βXi1+···+1βXi5+···.55The resulting model will represent an improvement to the extent that the average of the five standardized economy measures is a better predictor than the singlew.avg 1 5xi = 5 so that the data model becomesyi = α+βxw.avg +··· i= α+ 1γ1βXi1 +···+ 1γ5βXi5 +··· . 55γjXij, for i = 1,...,n,j=1We would like to estimate the relative coefficients γj from the data, but we cannot simply use classical regression, since this would then be equivalent to estimating a separate coefficient for each of the five predictors, and we have already established that not enough data are available to do a good job of this.Instead, one can set up a model for the γj’s:γj ∼ N(1,σγ2), for j = 1,...,5, (13.16)so that, in the model (13.15), the common coefficient β can be estimated classically, but the relative coefficients γj are part of a multilevel model. The hyperparameter σγ can be interpreted as follows:• If σγ = 0, the model reduces to the simple averaging (13.14): complete pooling
296 VARYING SLOPES AND NON-NESTED MODELS of the γj’s to the common value of 1, so that the combined predictor xw.avg issimply xavg, the average of the five individual X(j)’s.• If σγ = ∞, there is no pooling, with the individual coefficients 1γjβ estimated5separately using least squares.• When σγ is positive but finite, the γj’s are partially pooled, so that the fivepredictors xj have coefficients that are near each other but not identical.Depending on the amount of data available, σγ can be estimated as part of the model or set to a value such as 0.3 that constrains the γj’s to be fairly close to 1 and thus constrains the coefficients of the individual xj ’s toward each other in the data model (13.15).Connection to factor analysisA model can include multiplicative parameters for both modeling and computa- tional purposes. For example, we could predict the election outcome in year t in state s within region r[s] as5 j=1where X(0) is the matrix of state × year-level predictors, X(1) is the matrix of year-level predictors, and γ, δ, and  are national, regional, and statewide error terms.In this model, the auxiliary parameters α2 and α3 exist for purely computationalreasons, and they can be estimated, with the understanding that we are interestedonly in the products α2γt and α3δr,t. More interestingly, α1 serves both a compu-tational and modeling role—the β(1) parameters have a common N( 1 , σ2 ) model, j5mand α1 has the interpretation as the overall coefficient for the economic predictors. More generally, we can imagine K batches of predictors, with the data-levelregression model using a weighted average from each batch:y = X(0)β(0) + β1xw.avg, 1 + · · · + βkxw.avg, K + · · · ,where each predictor xw.avg is a combination of Jk individual predictors xjk: kyst = β(0)X(0) + α1 stβ(1)X(1) + α2γt + α3δ + st, j jt r[s],t1 Jki Jk ifor each k: xw.avg,k =This is equivalent to a regression model on the complete set of available predictors,x11,...,xJ11;x12,...,xJ22;...,x1K,...,xJKK, where the predictor xjk gets the co-efficient 1 γjkβk. Each batch of relative weights γ is then modeled hierarchically: Jkforeachk: γjk ∼N(1,σγ2k), forj=1,...,Jk,with the hyperparameters σγ k estimated from the data or set to low values such as 0.3.In this model, each combined predictor xw.avg, k represents a “factor” formed by a linear combination of the Jk individual predictors, βk represents the importance of that factor, and the γjk’s give the relative importance of the different components.As noted at the beginning of this section, these models are currently the subject of active research, and we suggest that they can serve as a motivation to specially tailored models for individual problems rather than as off-the-shelf solutions to generic multilevel problems with many predictors.γjkxjk, for i = 1,...,n.j=1
MORE COMPLEX MULTILEVEL MODELS 29713.7 More complex multilevel modelsThe models we have considered so far can be generalized in a variety of ways. Chapters 14 and 15 discuss multilevel logistic and generalized linear models. Other extensions within multilevel linear and generalized linear models include the fol- lowing:• Variances can vary, as parametric functions of input variables, and in a mul- tilevel way by allowing different variances for groups. For example, the model yi ∼ N(Xiβ,σi2), with σi = exp(Xiγ), allows the variance to depend on the predictors in a way that can be estimated from the data, and similarly, in a multilevel context, a model such as σi = exp(aj[i] + bxi) allows variances to vary by group. (It is natural to model the parameters σ on the log scale because they are restricted to be positive.)• Models with several factors can have many potential interactions, which them- selves can be modeled in a structured way, for example with larger variances for coefficients of interactions whose main effects are large. This is a model-based, multilevel version of general advice for classical regression modeling.• Regression models can be set up for multivariate outcomes, so that vectors of coefficients become matrices, with a data-level covariance matrix. These models become correspondingly more complex when multilevel factors are added.• Time series can be modeled in many ways going beyond simple autoregressions, and these parameters can vary by group with time-series cross-sectional data. This can be seen as a special case of non-nested groupings (for example, country × year), with calendar time being a group-level predictor.• One way to go beyond linearity is with nonparametric regression, with the sim- plest version being yi = g(Xi, θ) + i, and the function g being allowed to have some general form (for example, cubic splines, which are piecewise-continuous third-degree polynomials). Versions of such models can also be estimated using locally weighted regression, and again can be expanded to multilevel structures as appropriate.• More complicated models are appropriate to data with spatial or network struc- ture. These can be thought of as generalizations of multilevel models in which groups (for example, social networks) are not necessarily disjoint, and in which group membership can be continuous (some connections are stronger than oth- ers) rather than simply “in” or “out.”We do not discuss any of these models further here, but we wanted to bring them up to be clear that the particular models presented in this book are just the starting point to our general modeling approach.13.8 Bibliographic noteThe textbooks by Kreft and De Leeuw (1998), Raudenbush and Bryk (2002), and others discuss multilevel models with varying intercepts and slopes. For an early example, see Dempster, Rubin, and Tsutakawa (1981). Non-nested models are dis- cussed by Rasbash and Browne (2003). The flight simulator example comes from Gawron et al. (2003), and the latin square example comes from Snedecor and Cochran (1989).Models for covariance matrices have been presented by Barnard, McCulloch, and Meng (1996), Pinheiro and Bates (1996), Daniels and Kass (1999, 2001), Daniels and Pourahmadi (2002). Boscardin and Gelman (1996) discuss parametric models
298 VARYING SLOPES AND NON-NESTED MODELSfor unequal variances in multilevel linear regression. The scaled inverse-Wishart model we recommend comes from O’Malley and Zaslavsky (2005).The models for combining regression predictors discussed in Section 13.6 ap- pear in Witte et al. (1994), Greenland (2000), Gelman (2004b), and Gustafson and Greenland (2005). See also Hodges et al. (2005) and West (2003) on methods of including many predictors and interactions in a regression. Other work on select- ing and combining regression predictors in multilevel models includes Madigan and Raftery (1994), Hoeting et al. (1999), Chipman, George, and McCulloch (2001), and Dunson (2006). The election forecasting example is discussed in Gelman and King (1993) and Gelman et al. (2003, section 15.2); see Fair (1978), Rosenstone (1983), Campbell (1992), and Wlezien and Erikson (2004, 2005) for influential work in this area.Some references for hierarchical spatial and space-time models include Besag, York, and Mollie (1991), Waller et al. (1997), Besag and Higdon (1999), Wikle et al. (2001), and Bannerjee, Gelfand, and Carlin (2003). Jackson, Best, and Richardson (2006) discuss hierarchical models combining aggregate and survey data in public health. Datta et al. (1999) compare hierarchical time series models; see also Fay and Herriot (1979). Girosi and King (2005) present a multilevel model for estimating trends within demographic subgroups.For information on nonparametric methods such as lowess, splines, wavelets, haz- ard regression, generalized additive models, and regression trees, see Hastie, Tibshi- rani, and Friedman (2002), and, for examples in R, see Venables and Ripley (2002). Crainiceanu, Ruppert, and Wand (2005) fit spline models using Bugs. MacLehose et al. (2006) combine ideas of nonparametric and multilevel models.13.9 Exercises1. Fit a multilevel model to predict course evaluations from beauty and other pre- dictors in the beauty dataset (see Exercises 3.5, 4.8, and 12.6) allowing the intercept and coefficient for beauty to vary by course category:(a) Write the model in statistical notation.(b) Fit the model using lmer() and discuss the results: the coefficient estimates and the estimated standard deviation and correlation parameters. Identify each of the estimated parameters with the notation in your model from (a).(c) Display the estimated model graphically in plots that also include the data.2. Models for adjusting individual ratings: a committee of 10 persons is evaluat- ing 100 job applications. Each person on the committee reads 30 applications (structured so that each application is read by three people) and gives each a numerical rating between 1 and 10.(a) It would be natural to rate the applications based on their combined scores; however, there is a worry that different raters use different standards, and we would like to correct for this. Set up a model for the ratings (with parameters for the applicants and the raters).(b) It is possible that some persons on the committee show more variation than others in their ratings. Expand your model to allow for this.3. Non-nested model: continuing the Olympic ratings example from Exercise 11.3:(a) Write the notation for a non-nested multilevel model (varying across skaters and judges) for the technical merit ratings and fit using lmer().
EXERCISES 299(b) Fit the model in (a) using the artistic impression ratings.(c) Display your results for both outcomes graphically.(d) Use posterior predictive checks to investigate model fit in (a) and (b).4. Models with unequal variances: the folder age.guessing contains a dataset from Gelman and Nolan (2002) from a classroom demonstration in which 10 groups of students guess the ages of 10 different persons based on photographs. The dataset also includes the true ages of the people in the photographs.Set up a non-nested model to these data, including a coefficient for each of the persons in the photos (indicating their apparent age), a coefficient for each of the 10 groups (indicating potential systematic patterns of groups guessing high or low), and a separate error variance for each group (so that some groups are more consistent than others).5. Return to the CD4 data introduced from Exercise 11.4.(a) Extend the model in Exercise 12.2 to allow for varying slopes for the time predictor.(b) Next fit a model that does not allow for varying slopes but does allow for different coefficients for each time point (rather than fitting the linear trend).(c) Compare the results of these models both numerically and graphically.6. Using the time-series cross-sectional dataset you worked with in Exercise 11.2, fit the model you formulated in part (c) of that exercise.
